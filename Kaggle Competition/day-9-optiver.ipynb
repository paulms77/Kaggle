{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd264c23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-27T16:13:25.398968Z",
     "iopub.status.busy": "2021-09-27T16:13:25.398253Z",
     "iopub.status.idle": "2021-09-27T16:13:26.638050Z",
     "shell.execute_reply": "2021-09-27T16:13:26.637291Z",
     "shell.execute_reply.started": "2021-09-27T14:24:32.059673Z"
    },
    "papermill": {
     "duration": 1.290871,
     "end_time": "2021-09-27T16:13:26.638234",
     "exception": false,
     "start_time": "2021-09-27T16:13:25.347363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce04c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:13:26.771106Z",
     "iopub.status.busy": "2021-09-27T16:13:26.733302Z",
     "iopub.status.idle": "2021-09-27T16:13:26.794914Z",
     "shell.execute_reply": "2021-09-27T16:13:26.794293Z"
    },
    "papermill": {
     "duration": 0.12435,
     "end_time": "2021-09-27T16:13:26.795069",
     "exception": false,
     "start_time": "2021-09-27T16:13:26.670719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_waph1(df):\n",
    "    wap1_mean = df['wap1'].mean()\n",
    "    waph1 = df['wap1'].apply(lambda x:x-wap1_mean)\n",
    "    return waph1\n",
    "    \n",
    "def calc_waph2(df):\n",
    "    wap2_mean = df['wap2'].mean()\n",
    "    waph2 = df['wap2'].apply(lambda x:x-wap2_mean)\n",
    "    return waph2\n",
    "    \n",
    "def calc_waph3(df):\n",
    "    wap3_mean = df['wap3'].mean()\n",
    "    waph3 = df['wap3'].apply(lambda x:x-wap3_mean)\n",
    "    return waph3\n",
    "    \n",
    "def calc_waph4(df):\n",
    "    wap4_mean = df['wap4'].mean()\n",
    "    waph4 = df['wap4'].apply(lambda x:x-wap4_mean)\n",
    "    return waph4\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def historical_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2)/len(series))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def encode_mean(column, df):\n",
    "    avg = df.groupby('time_id')[column].transform('mean')\n",
    "    return np.abs(df[column].sub(avg).div(avg))\n",
    "\n",
    "def pressure_compute1(df):\n",
    "    df['mid_price1'] = (df['bid_price1'] + df['ask_price1'])/2\n",
    "    sell_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price1'])\n",
    "    sell_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price2'])\n",
    "    buy_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price1'])\n",
    "    buy_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price2'])\n",
    "    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df['ask_size1']+ \\\n",
    "    sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df['ask_size2']\n",
    "    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df['bid_size1']+ \\\n",
    "    buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df['bid_size2']\n",
    "    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\n",
    "    return pressure_ratio\n",
    "\n",
    "def pressure_compute2(df):\n",
    "    df['mid_price2'] = (df['bid_price2'] + df['ask_price2'])/2\n",
    "    sell_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['ask_price1'])\n",
    "    sell_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['ask_price2'])\n",
    "    buy_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['bid_price1'])\n",
    "    buy_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['bid_price2'])\n",
    "    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df['ask_size1']+ \\\n",
    "    sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df['ask_size2']\n",
    "    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df['bid_size1']+ \\\n",
    "    buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df['bid_size2']\n",
    "    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\n",
    "    return pressure_ratio\n",
    "\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train,test\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    #df['waph1'] = calc_waph1(df)\n",
    "    #df['waph2'] = calc_waph2(df)\n",
    "    #df['waph3'] = calc_waph3(df)\n",
    "    #df['waph4'] = calc_waph4(df)\n",
    "    #df['relative_wap_balance1'] = (df['wap1'] - df['wap2'])/( df['wap1'] + df['wap2'])\n",
    "    #df[\"relative_wap_balance2\"] = (df['wap3'] - df['wap4'])/( df['wap3'] + df['wap4'])\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    #df['wap_balance2'] = abs(df['wap3'] - df['wap4'])\n",
    "\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['bid_ask_spread1'] = abs(df['bid_price1'] - df['ask_price1']) # Spread가 좁을수록 판매가 easy -> 거래가 많아짐\n",
    "    df['bid_ask_spread2'] = abs(df['bid_price2'] - df['ask_price2'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    #df['pressure_ratio1'] = pressure_compute1(df)\n",
    "    #df['pressure_ratio2'] = pressure_compute2(df)\n",
    "    \n",
    "    #Calculate Volume imbalance \n",
    "    #df['depth_imbalance1'] = (df['ask_size1'] - df[\"bid_size1\"])/(df['ask_size1'] + df[\"bid_size1\"])\n",
    "    #df['depth_imbalance2'] = (df['ask_size2'] - df[\"bid_size2\"])/(df['ask_size2'] + df[\"bid_size2\"])\n",
    "    \n",
    "    def premad(x):\n",
    "        return np.median(np.absolute(x-np.median(x,axis=0)),axis=0)\n",
    "    \n",
    "    def iqr_func5(x):\n",
    "        low = 5\n",
    "        high = 95\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    \n",
    "    def iqr_func25(x):\n",
    "        low = 25\n",
    "        high = 75\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    \n",
    "    def iqr_func15(x):\n",
    "        low = 15\n",
    "        high = 85\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    \n",
    "    def iqr_func5(x):\n",
    "        low = 5\n",
    "        high = 95\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "\n",
    "    def preskew(x):\n",
    "        return skew(x)\n",
    "\n",
    "    def prekurt(x):\n",
    "        return kurtosis(x,fisher=True)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'bid_price1':['max','min','last','first'],\n",
    "        'bid_price2':['max','min','last','first'],\n",
    "        'ask_price1':['max','min','last','first'],\n",
    "        'ask_price2':['max','min','last','first'],\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        #'waph1':[np.sum, np.std],\n",
    "        #'waph2':[np.sum, np.std],\n",
    "        #'waph3':[np.sum, np.std],\n",
    "        #'waph4':[np.sum, np.std],\n",
    "        #'relative_wap_balance1':[np.sum, np.max],\n",
    "        #'depth_imbalance1':[np.sum, np.max],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        #'bid_price1':['max','min','last','first'],\n",
    "        #'bid_price2':['max','min','last','first'],\n",
    "        #'ask_price1':['max','min','last','first'],\n",
    "        #'ask_price2':['max','min','last','first'],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "        'price':['max','min','last','first'],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "        'price':['max','min','last','first'],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5054aa34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:13:26.885869Z",
     "iopub.status.busy": "2021-09-27T16:13:26.869247Z",
     "iopub.status.idle": "2021-09-27T16:13:26.892131Z",
     "shell.execute_reply": "2021-09-27T16:13:26.892646Z",
     "shell.execute_reply.started": "2021-09-27T14:26:44.144533Z"
    },
    "papermill": {
     "duration": 0.065753,
     "end_time": "2021-09-27T16:13:26.892815",
     "exception": false,
     "start_time": "2021-09-27T16:13:26.827062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# data directory\\ndata_dir = \\'../input/optiver-realized-volatility-prediction/\\'\\n\\n# Function to calculate first WAP\\ndef calc_wap1(df):\\n    wap = (df[\\'bid_price1\\'] * df[\\'ask_size1\\'] + df[\\'ask_price1\\'] * df[\\'bid_size1\\']) / (df[\\'bid_size1\\'] + df[\\'ask_size1\\'])\\n    return wap\\n\\n# Function to calculate second WAP\\ndef calc_wap2(df):\\n    wap = (df[\\'bid_price2\\'] * df[\\'ask_size2\\'] + df[\\'ask_price2\\'] * df[\\'bid_size2\\']) / (df[\\'bid_size2\\'] + df[\\'ask_size2\\'])\\n    return wap\\n\\ndef calc_wap3(df):\\n    wap = (df[\\'bid_price1\\'] * df[\\'bid_size1\\'] + df[\\'ask_price1\\'] * df[\\'ask_size1\\']) / (df[\\'bid_size1\\'] + df[\\'ask_size1\\'])\\n    return wap\\n\\ndef calc_wap4(df):\\n    wap = (df[\\'bid_price2\\'] * df[\\'bid_size2\\'] + df[\\'ask_price2\\'] * df[\\'ask_size2\\']) / (df[\\'bid_size2\\'] + df[\\'ask_size2\\'])\\n    return wap\\n\\ndef calc_waph1(df):\\n    wap1_mean = df[\\'wap1\\'].mean()\\n    waph1 = df[\\'wap1\\'].apply(lambda x:x-wap1_mean)\\n    return waph1\\n    \\ndef calc_waph2(df):\\n    wap2_mean = df[\\'wap2\\'].mean()\\n    waph2 = df[\\'wap2\\'].apply(lambda x:x-wap2_mean)\\n    return waph2\\n    \\ndef calc_waph3(df):\\n    wap3_mean = df[\\'wap3\\'].mean()\\n    waph3 = df[\\'wap3\\'].apply(lambda x:x-wap3_mean)\\n    return waph3\\n    \\ndef calc_waph4(df):\\n    wap4_mean = df[\\'wap4\\'].mean()\\n    waph4 = df[\\'wap4\\'].apply(lambda x:x-wap4_mean)\\n    return waph4\\n\\ndef log_return(series):\\n    return np.log(series).diff()\\n\\ndef realized_volatility(series):\\n    return np.sqrt(np.sum(series**2))\\n\\ndef historical_volatility(series):\\n    return np.sqrt(np.sum(series**2)/len(series))\\n\\ndef count_unique(series):\\n    return len(np.unique(series))\\n\\ndef encode_mean(column, df):\\n    avg = df.groupby(\\'time_id\\')[column].transform(\\'mean\\')\\n    return np.abs(df[column].sub(avg).div(avg))\\n\\ndef pressure_compute1(df):\\n    df[\\'mid_price1\\'] = (df[\\'bid_price1\\'] + df[\\'ask_price1\\'])/2\\n    sell_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df[\\'ask_price1\\'])\\n    sell_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df[\\'ask_price2\\'])\\n    buy_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df[\\'bid_price1\\'])\\n    buy_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df[\\'bid_price2\\'])\\n    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df[\\'ask_size1\\']+     sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df[\\'ask_size2\\']\\n    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df[\\'bid_size1\\']+     buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df[\\'bid_size2\\']\\n    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\\n    return pressure_ratio\\n\\ndef pressure_compute2(df):\\n    df[\\'mid_price2\\'] = (df[\\'bid_price2\\'] + df[\\'ask_price2\\'])/2\\n    sell_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df[\\'ask_price1\\'])\\n    sell_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df[\\'ask_price2\\'])\\n    buy_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df[\\'bid_price1\\'])\\n    buy_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df[\\'bid_price2\\'])\\n    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df[\\'ask_size1\\']+     sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df[\\'ask_size2\\']\\n    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df[\\'bid_size1\\']+     buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df[\\'bid_size2\\']\\n    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\\n    return pressure_ratio\\n\\ndef read_train_test():\\n    train = pd.read_csv(\\'../input/optiver-realized-volatility-prediction/train.csv\\')\\n    test = pd.read_csv(\\'../input/optiver-realized-volatility-prediction/test.csv\\')\\n    # Create a key to merge with book and trade data\\n    train[\\'row_id\\'] = train[\\'stock_id\\'].astype(str) + \\'-\\' + train[\\'time_id\\'].astype(str)\\n    test[\\'row_id\\'] = test[\\'stock_id\\'].astype(str) + \\'-\\' + test[\\'time_id\\'].astype(str)\\n    print(f\\'Our training set has {train.shape[0]} rows\\')\\n    return train,test\\n\\ndef book_preprocessor(file_path):\\n    df = pd.read_parquet(file_path)\\n    \\n    # Calculate Wap\\n    df[\\'wap1\\'] = calc_wap1(df)\\n    df[\\'wap2\\'] = calc_wap2(df)\\n    df[\\'wap3\\'] = calc_wap3(df)\\n    df[\\'wap4\\'] = calc_wap4(df)\\n    #df[\\'waph1\\'] = calc_waph1(df)\\n    #df[\\'waph2\\'] = calc_waph2(df)\\n    #df[\\'waph3\\'] = calc_waph3(df)\\n    #df[\\'waph4\\'] = calc_waph4(df)\\n    #df[\\'relative_wap_balance1\\'] = (df[\\'wap1\\'] - df[\\'wap2\\'])/( df[\\'wap1\\'] + df[\\'wap2\\'])\\n    #df[\"relative_wap_balance2\"] = (df[\\'wap3\\'] - df[\\'wap4\\'])/( df[\\'wap3\\'] + df[\\'wap4\\'])\\n    \\n    # Calculate log returns\\n    df[\\'log_return1\\'] = df.groupby([\\'time_id\\'])[\\'wap1\\'].apply(log_return)\\n    df[\\'log_return2\\'] = df.groupby([\\'time_id\\'])[\\'wap2\\'].apply(log_return)\\n    df[\\'log_return3\\'] = df.groupby([\\'time_id\\'])[\\'wap3\\'].apply(log_return)\\n    df[\\'log_return4\\'] = df.groupby([\\'time_id\\'])[\\'wap4\\'].apply(log_return)\\n    \\n    # Calculate wap balance\\n    df[\\'wap_balance\\'] = abs(df[\\'wap1\\'] - df[\\'wap2\\'])\\n    #df[\\'wap_balance2\\'] = abs(df[\\'wap3\\'] - df[\\'wap4\\'])\\n\\n    \\n    # Calculate spread\\n    df[\\'price_spread\\'] = (df[\\'ask_price1\\'] - df[\\'bid_price1\\']) / ((df[\\'ask_price1\\'] + df[\\'bid_price1\\']) / 2)\\n    df[\\'price_spread2\\'] = (df[\\'ask_price2\\'] - df[\\'bid_price2\\']) / ((df[\\'ask_price2\\'] + df[\\'bid_price2\\']) / 2)\\n    df[\\'bid_spread\\'] = df[\\'bid_price1\\'] - df[\\'bid_price2\\']\\n    df[\\'ask_spread\\'] = df[\\'ask_price1\\'] - df[\\'ask_price2\\']\\n    df[\"bid_ask_spread\"] = abs(df[\\'bid_spread\\'] - df[\\'ask_spread\\'])\\n    df[\\'bid_ask_spread1\\'] = abs(df[\\'bid_price1\\'] - df[\\'ask_price1\\']) # Spread가 좁을수록 판매가 easy -> 거래가 많아짐\\n    df[\\'bid_ask_spread2\\'] = abs(df[\\'bid_price2\\'] - df[\\'ask_price2\\'])\\n    df[\\'total_volume\\'] = (df[\\'ask_size1\\'] + df[\\'ask_size2\\']) + (df[\\'bid_size1\\'] + df[\\'bid_size2\\'])\\n    df[\\'volume_imbalance\\'] = abs((df[\\'ask_size1\\'] + df[\\'ask_size2\\']) - (df[\\'bid_size1\\'] + df[\\'bid_size2\\']))\\n    \\n    #df[\\'pressure_ratio1\\'] = pressure_compute1(df)\\n    #df[\\'pressure_ratio2\\'] = pressure_compute2(df)\\n    \\n    #Calculate Volume imbalance \\n    #df[\\'depth_imbalance1\\'] = (df[\\'ask_size1\\'] - df[\"bid_size1\"])/(df[\\'ask_size1\\'] + df[\"bid_size1\"])\\n    #df[\\'depth_imbalance2\\'] = (df[\\'ask_size2\\'] - df[\"bid_size2\"])/(df[\\'ask_size2\\'] + df[\"bid_size2\"])\\n    \\n    def premad(x):\\n        return np.median(np.absolute(x-np.median(x,axis=0)),axis=0)\\n    \\n    def iqr_func25(x):\\n        low = 25\\n        high = 75\\n        q3, q1 = np.percentile(x, [low, high])\\n        iqr = q3 - q1\\n        return iqr\\n    \\n    def iqr_func15(x):\\n        low = 15\\n        high = 85\\n        q3, q1 = np.percentile(x, [low, high])\\n        iqr = q3 - q1\\n        return iqr\\n    \\n    def iqr_func5(x):\\n        low = 5\\n        high = 95\\n        q3, q1 = np.percentile(x, [low, high])\\n        iqr = q3 - q1\\n        return iqr\\n\\n    def preskew(x):\\n        return skew(x)\\n\\n    def prekurt(x):\\n        return kurtosis(x,fisher=True)\\n    \\n    # Dict for aggregations\\n    create_feature_dict = {\\n        \\'bid_price1\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'bid_price2\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'ask_price1\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'ask_price2\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'wap1\\': [np.sum, np.std],\\n        \\'wap2\\': [np.sum, np.std],\\n        \\'wap3\\': [np.sum, np.std],\\n        \\'wap4\\': [np.sum, np.std],\\n        #\\'waph1\\':[np.sum, np.std],\\n        #\\'waph2\\':[np.sum, np.std],\\n        #\\'waph3\\':[np.sum, np.std],\\n        #\\'waph4\\':[np.sum, np.std],\\n        #\\'relative_wap_balance1\\':[np.sum, np.max],\\n        #\\'depth_imbalance1\\':[np.sum, np.max],\\n        \\'log_return1\\': [realized_volatility],\\n        \\'log_return2\\': [realized_volatility],\\n        \\'log_return3\\': [realized_volatility],\\n        \\'log_return4\\': [realized_volatility],\\n        \\'wap_balance\\': [np.sum, np.max],\\n        \\'price_spread\\':[np.sum, np.max],\\n        \\'price_spread2\\':[np.sum, np.max],\\n        \\'bid_spread\\':[np.sum, np.max],\\n        \\'ask_spread\\':[np.sum, np.max],\\n        \\'total_volume\\':[np.sum, np.max],\\n        \\'volume_imbalance\\':[np.sum, np.max],\\n        \"bid_ask_spread\":[np.sum,  np.max],\\n    }\\n    create_feature_dict_time = {\\n        \\'log_return1\\': [realized_volatility],\\n        \\'log_return2\\': [realized_volatility],\\n        \\'log_return3\\': [realized_volatility],\\n        \\'log_return4\\': [realized_volatility],\\n    }\\n    \\n    # Function to get group stats for different windows (seconds in bucket)\\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\\n        # Group by the window\\n        df_feature = df[df[\\'seconds_in_bucket\\'] >= seconds_in_bucket].groupby([\\'time_id\\']).agg(fe_dict).reset_index()\\n        # Rename columns joining suffix\\n        df_feature.columns = [\\'_\\'.join(col) for col in df_feature.columns]\\n        # Add a suffix to differentiate windows\\n        if add_suffix:\\n            df_feature = df_feature.add_suffix(\\'_\\' + str(seconds_in_bucket))\\n        return df_feature\\n    \\n    # Get the stats for different windows\\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\\n\\n    # Merge all\\n    df_feature = df_feature.merge(df_feature_500, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__500\\')\\n    df_feature = df_feature.merge(df_feature_400, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__400\\')\\n    df_feature = df_feature.merge(df_feature_300, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__300\\')\\n    df_feature = df_feature.merge(df_feature_200, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__200\\')\\n    df_feature = df_feature.merge(df_feature_100, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__100\\')\\n    # Drop unnecesary time_ids\\n    df_feature.drop([\\'time_id__500\\',\\'time_id__400\\', \\'time_id__300\\', \\'time_id__200\\',\\'time_id__100\\'], axis = 1, inplace = True)\\n    \\n    \\n    # Create row_id so we can merge\\n    stock_id = file_path.split(\\'=\\')[1]\\n    df_feature[\\'row_id\\'] = df_feature[\\'time_id_\\'].apply(lambda x: f\\'{stock_id}-{x}\\')\\n    df_feature.drop([\\'time_id_\\'], axis = 1, inplace = True)\\n    return df_feature\\n\\n# Function to preprocess trade data (for each stock id)\\ndef trade_preprocessor(file_path):\\n    df = pd.read_parquet(file_path)\\n    df[\\'log_return\\'] = df.groupby(\\'time_id\\')[\\'price\\'].apply(log_return)\\n    df[\\'amount\\']=df[\\'price\\']*df[\\'size\\']\\n    # Dict for aggregations\\n    create_feature_dict = {\\n        \\'log_return\\':[realized_volatility],\\n        \\'seconds_in_bucket\\':[count_unique],\\n        \\'size\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'order_count\\':[np.sum,np.max],\\n        \\'amount\\':[np.sum,np.max, np.min],\\n        \\'price\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n    }\\n    create_feature_dict_time = {\\n        \\'log_return\\':[realized_volatility],\\n        \\'seconds_in_bucket\\':[count_unique],\\n        \\'size\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n        \\'order_count\\':[np.sum],\\n        \\'price\\':[np.sum,\\'max\\',\\'min\\',\\'last\\',\\'first\\'],\\n    }\\n    # Function to get group stats for different windows (seconds in bucket)\\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\\n        # Group by the window\\n        df_feature = df[df[\\'seconds_in_bucket\\'] >= seconds_in_bucket].groupby([\\'time_id\\']).agg(fe_dict).reset_index()\\n        # Rename columns joining suffix\\n        df_feature.columns = [\\'_\\'.join(col) for col in df_feature.columns]\\n        # Add a suffix to differentiate windows\\n        if add_suffix:\\n            df_feature = df_feature.add_suffix(\\'_\\' + str(seconds_in_bucket))\\n        return df_feature\\n    \\n\\n    # Get the stats for different windows\\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\\n    \\n    def tendency(price, vol):    \\n        df_diff = np.diff(price)\\n        val = (df_diff/price[1:])*100\\n        power = np.sum(val*vol[1:])\\n        return(power)\\n    \\n    lis = []\\n    for n_time_id in df[\\'time_id\\'].unique():\\n        df_id = df[df[\\'time_id\\'] == n_time_id]        \\n        tendencyV = tendency(df_id[\\'price\\'].values, df_id[\\'size\\'].values)      \\n        f_max = np.sum(df_id[\\'price\\'].values > np.mean(df_id[\\'price\\'].values))\\n        f_min = np.sum(df_id[\\'price\\'].values < np.mean(df_id[\\'price\\'].values))\\n        df_max =  np.sum(np.diff(df_id[\\'price\\'].values) > 0)\\n        df_min =  np.sum(np.diff(df_id[\\'price\\'].values) < 0)\\n        # new\\n        abs_diff = np.median(np.abs( df_id[\\'price\\'].values - np.mean(df_id[\\'price\\'].values)))        \\n        energy = np.mean(df_id[\\'price\\'].values**2)\\n        iqr_p75 = np.percentile(df_id[\\'price\\'].values,75) - np.percentile(df_id[\\'price\\'].values,25)\\n        #iqr_p85 = np.percentile(df_id[\\'price\\'].values,85) - np.percentile(df_id[\\'price\\'].values,15)\\n        #iqr_p95 = np.percentile(df_id[\\'price\\'].values,95) - np.percentile(df_id[\\'price\\'].values,5)\\n        \\n        # vol vars\\n        abs_diff_v = np.median(np.abs( df_id[\\'size\\'].values - np.mean(df_id[\\'size\\'].values)))        \\n        energy_v = np.sum(df_id[\\'size\\'].values**2)\\n        iqr_p_v75 = np.percentile(df_id[\\'size\\'].values,75) - np.percentile(df_id[\\'size\\'].values,25)\\n        #iqr_p_v85 = np.percentile(df_id[\\'size\\'].values,85) - np.percentile(df_id[\\'size\\'].values,15)\\n        #iqr_p_v95 = np.percentile(df_id[\\'size\\'].values,95) - np.percentile(df_id[\\'size\\'].values,5)\\n        \\n        lis.append({\\'time_id\\':n_time_id,\\'tendency\\':tendencyV,\\'f_max\\':f_max,\\'f_min\\':f_min,\\'df_max\\':df_max,\\'df_min\\':df_min,\\n                   \\'abs_diff\\':abs_diff,\\'energy\\':energy,\\'iqr_p75\\':iqr_p75,\\'abs_diff_v\\':abs_diff_v,\\'energy_v\\':energy_v,\\'iqr_p_v75\\':iqr_p_v75})\\n    \\n    df_lr = pd.DataFrame(lis)\\n        \\n   \\n    df_feature = df_feature.merge(df_lr, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id\\')\\n    \\n    # Merge all\\n    df_feature = df_feature.merge(df_feature_500, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__500\\')\\n    df_feature = df_feature.merge(df_feature_400, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__400\\')\\n    df_feature = df_feature.merge(df_feature_300, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__300\\')\\n    df_feature = df_feature.merge(df_feature_200, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__200\\')\\n    df_feature = df_feature.merge(df_feature_100, how = \\'left\\', left_on = \\'time_id_\\', right_on = \\'time_id__100\\')\\n    # Drop unnecesary time_ids\\n    df_feature.drop([\\'time_id__500\\',\\'time_id__400\\', \\'time_id__300\\', \\'time_id__200\\',\\'time_id\\',\\'time_id__100\\'], axis = 1, inplace = True)\\n    \\n    \\n    df_feature = df_feature.add_prefix(\\'trade_\\')\\n    stock_id = file_path.split(\\'=\\')[1]\\n    df_feature[\\'row_id\\'] = df_feature[\\'trade_time_id_\\'].apply(lambda x:f\\'{stock_id}-{x}\\')\\n    df_feature.drop([\\'trade_time_id_\\'], axis = 1, inplace = True)\\n    return df_feature\\n\\n# Function to get group stats for the stock_id and time_id\\ndef get_time_stock(df):\\n    vol_cols = [\\'log_return1_realized_volatility\\', \\'log_return2_realized_volatility\\', \\'log_return1_realized_volatility_400\\', \\'log_return2_realized_volatility_400\\', \\n                \\'log_return1_realized_volatility_300\\', \\'log_return2_realized_volatility_300\\', \\'log_return1_realized_volatility_200\\', \\'log_return2_realized_volatility_200\\', \\n                \\'trade_log_return_realized_volatility\\', \\'trade_log_return_realized_volatility_400\\', \\'trade_log_return_realized_volatility_300\\', \\'trade_log_return_realized_volatility_200\\']\\n\\n\\n    # Group by the stock id\\n    df_stock_id = df.groupby([\\'stock_id\\'])[vol_cols].agg([\\'sum\\',\\'mean\\', \\'std\\', \\'max\\', \\'min\\', ]).reset_index()\\n    # Rename columns joining suffix\\n    df_stock_id.columns = [\\'_\\'.join(col) for col in df_stock_id.columns]\\n    df_stock_id = df_stock_id.add_suffix(\\'_\\' + \\'stock\\')\\n\\n    # Group by the stock id\\n    df_time_id = df.groupby([\\'time_id\\'])[vol_cols].agg([\\'sum\\',\\'mean\\', \\'std\\', \\'max\\', \\'min\\', ]).reset_index()\\n    # Rename columns joining suffix\\n    df_time_id.columns = [\\'_\\'.join(col) for col in df_time_id.columns]\\n    df_time_id = df_time_id.add_suffix(\\'_\\' + \\'time\\')\\n    \\n    # Merge with original dataframe\\n    df = df.merge(df_stock_id, how = \\'left\\', left_on = [\\'stock_id\\'], right_on = [\\'stock_id__stock\\'])\\n    df = df.merge(df_time_id, how = \\'left\\', left_on = [\\'time_id\\'], right_on = [\\'time_id__time\\'])\\n    df.drop([\\'stock_id__stock\\', \\'time_id__time\\'], axis = 1, inplace = True)\\n    return df\\n    \\n# Funtion to make preprocessing function in parallel (for each stock id)\\ndef preprocessor(list_stock_ids, is_train = True):\\n    \\n    # Parrallel for loop\\n    def for_joblib(stock_id):\\n        # Train\\n        if is_train:\\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\\n        # Test\\n        else:\\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\\n    \\n        # Preprocess book and trade data and merge them\\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = \\'row_id\\', how = \\'left\\')\\n        \\n        # Return the merge dataframe\\n        return df_tmp\\n    \\n    # Use parallel api to call paralle for loop\\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\\n    # Concatenate all the dataframes that return from Parallel\\n    df = pd.concat(df, ignore_index = True)\\n    return df\\n\\n# Function to calculate the root mean squared percentage error\\ndef rmspe(y_true, y_pred):\\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\\n\\n# Function to early stop with root mean squared percentage error\\ndef feval_rmspe(y_pred, lgb_train):\\n    y_true = lgb_train.get_label()\\n    return \\'RMSPE\\', rmspe(y_true, y_pred), False\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_waph1(df):\n",
    "    wap1_mean = df['wap1'].mean()\n",
    "    waph1 = df['wap1'].apply(lambda x:x-wap1_mean)\n",
    "    return waph1\n",
    "    \n",
    "def calc_waph2(df):\n",
    "    wap2_mean = df['wap2'].mean()\n",
    "    waph2 = df['wap2'].apply(lambda x:x-wap2_mean)\n",
    "    return waph2\n",
    "    \n",
    "def calc_waph3(df):\n",
    "    wap3_mean = df['wap3'].mean()\n",
    "    waph3 = df['wap3'].apply(lambda x:x-wap3_mean)\n",
    "    return waph3\n",
    "    \n",
    "def calc_waph4(df):\n",
    "    wap4_mean = df['wap4'].mean()\n",
    "    waph4 = df['wap4'].apply(lambda x:x-wap4_mean)\n",
    "    return waph4\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def historical_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2)/len(series))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def encode_mean(column, df):\n",
    "    avg = df.groupby('time_id')[column].transform('mean')\n",
    "    return np.abs(df[column].sub(avg).div(avg))\n",
    "\n",
    "def pressure_compute1(df):\n",
    "    df['mid_price1'] = (df['bid_price1'] + df['ask_price1'])/2\n",
    "    sell_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price1'])\n",
    "    sell_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price2'])\n",
    "    buy_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price1'])\n",
    "    buy_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price2'])\n",
    "    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df['ask_size1']+ \\\n",
    "    sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df['ask_size2']\n",
    "    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df['bid_size1']+ \\\n",
    "    buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df['bid_size2']\n",
    "    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\n",
    "    return pressure_ratio\n",
    "\n",
    "def pressure_compute2(df):\n",
    "    df['mid_price2'] = (df['bid_price2'] + df['ask_price2'])/2\n",
    "    sell_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['ask_price1'])\n",
    "    sell_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['ask_price2'])\n",
    "    buy_percent_with_mid_price1 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['bid_price1'])\n",
    "    buy_percent_with_mid_price2 = df[\"mid_price2\"]/(df[\"mid_price2\"]-df['bid_price2'])\n",
    "    sell_pressure = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df['ask_size1']+ \\\n",
    "    sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df['ask_size2']\n",
    "    buy_pressure = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df['bid_size1']+ \\\n",
    "    buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df['bid_size2']\n",
    "    pressure_ratio = np.log(buy_pressure) - np.log(sell_pressure)\n",
    "    return pressure_ratio\n",
    "\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train,test\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    #df['waph1'] = calc_waph1(df)\n",
    "    #df['waph2'] = calc_waph2(df)\n",
    "    #df['waph3'] = calc_waph3(df)\n",
    "    #df['waph4'] = calc_waph4(df)\n",
    "    #df['relative_wap_balance1'] = (df['wap1'] - df['wap2'])/( df['wap1'] + df['wap2'])\n",
    "    #df[\"relative_wap_balance2\"] = (df['wap3'] - df['wap4'])/( df['wap3'] + df['wap4'])\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    #df['wap_balance2'] = abs(df['wap3'] - df['wap4'])\n",
    "\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['bid_ask_spread1'] = abs(df['bid_price1'] - df['ask_price1']) # Spread가 좁을수록 판매가 easy -> 거래가 많아짐\n",
    "    df['bid_ask_spread2'] = abs(df['bid_price2'] - df['ask_price2'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    #df['pressure_ratio1'] = pressure_compute1(df)\n",
    "    #df['pressure_ratio2'] = pressure_compute2(df)\n",
    "    \n",
    "    #Calculate Volume imbalance \n",
    "    #df['depth_imbalance1'] = (df['ask_size1'] - df[\"bid_size1\"])/(df['ask_size1'] + df[\"bid_size1\"])\n",
    "    #df['depth_imbalance2'] = (df['ask_size2'] - df[\"bid_size2\"])/(df['ask_size2'] + df[\"bid_size2\"])\n",
    "    \n",
    "    def premad(x):\n",
    "        return np.median(np.absolute(x-np.median(x,axis=0)),axis=0)\n",
    "    \n",
    "    def iqr_func25(x):\n",
    "        low = 25\n",
    "        high = 75\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    \n",
    "    def iqr_func15(x):\n",
    "        low = 15\n",
    "        high = 85\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "    \n",
    "    def iqr_func5(x):\n",
    "        low = 5\n",
    "        high = 95\n",
    "        q3, q1 = np.percentile(x, [low, high])\n",
    "        iqr = q3 - q1\n",
    "        return iqr\n",
    "\n",
    "    def preskew(x):\n",
    "        return skew(x)\n",
    "\n",
    "    def prekurt(x):\n",
    "        return kurtosis(x,fisher=True)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'bid_price1':[np.sum,'max','min','last','first'],\n",
    "        'bid_price2':[np.sum,'max','min','last','first'],\n",
    "        'ask_price1':[np.sum,'max','min','last','first'],\n",
    "        'ask_price2':[np.sum,'max','min','last','first'],\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        #'waph1':[np.sum, np.std],\n",
    "        #'waph2':[np.sum, np.std],\n",
    "        #'waph3':[np.sum, np.std],\n",
    "        #'waph4':[np.sum, np.std],\n",
    "        #'relative_wap_balance1':[np.sum, np.max],\n",
    "        #'depth_imbalance1':[np.sum, np.max],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum,'max','min','last','first'],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max, np.min],\n",
    "        'price':[np.sum,'max','min','last','first'],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum,'max','min','last','first'],\n",
    "        'order_count':[np.sum],\n",
    "        'price':[np.sum,'max','min','last','first'],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p75 = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        #iqr_p85 = np.percentile(df_id['price'].values,85) - np.percentile(df_id['price'].values,15)\n",
    "        #iqr_p95 = np.percentile(df_id['price'].values,95) - np.percentile(df_id['price'].values,5)\n",
    "        \n",
    "        # vol vars\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v75 = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        #iqr_p_v85 = np.percentile(df_id['size'].values,85) - np.percentile(df_id['size'].values,15)\n",
    "        #iqr_p_v95 = np.percentile(df_id['size'].values,95) - np.percentile(df_id['size'].values,5)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p75':iqr_p75,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v75':iqr_p_v75})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['sum','mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['sum','mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9096cd9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:13:26.969480Z",
     "iopub.status.busy": "2021-09-27T16:13:26.968681Z",
     "iopub.status.idle": "2021-09-27T16:56:49.228765Z",
     "shell.execute_reply": "2021-09-27T16:56:49.229263Z",
     "shell.execute_reply.started": "2021-09-27T14:26:44.823373Z"
    },
    "papermill": {
     "duration": 2602.302705,
     "end_time": "2021-09-27T16:56:49.229471",
     "exception": false,
     "start_time": "2021-09-27T16:13:26.926766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 43.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "#train =pd.read_pickle(\"../input/optiver006/train.pkl\")\n",
    "train,test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train1=train\n",
    "test1=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8720ac21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.311340Z",
     "iopub.status.busy": "2021-09-27T16:56:49.310569Z",
     "iopub.status.idle": "2021-09-27T16:56:49.329842Z",
     "shell.execute_reply": "2021-09-27T16:56:49.330375Z"
    },
    "papermill": {
     "duration": 0.065355,
     "end_time": "2021-09-27T16:56:49.330581",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.265226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb41b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.408913Z",
     "iopub.status.busy": "2021-09-27T16:56:49.408211Z",
     "iopub.status.idle": "2021-09-27T16:56:49.430364Z",
     "shell.execute_reply": "2021-09-27T16:56:49.429782Z"
    },
    "papermill": {
     "duration": 0.06523,
     "end_time": "2021-09-27T16:56:49.430524",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.365294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60840d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.504520Z",
     "iopub.status.busy": "2021-09-27T16:56:49.503840Z",
     "iopub.status.idle": "2021-09-27T16:56:49.506364Z",
     "shell.execute_reply": "2021-09-27T16:56:49.506880Z"
    },
    "papermill": {
     "duration": 0.041407,
     "end_time": "2021-09-27T16:56:49.507057",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.465650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train.columns[train.columns.str.contains('_last')]\n",
    "#price_last = ['bid_price1_last','bid_price2_last','ask_price1_last','ask_price2_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76831fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.580811Z",
     "iopub.status.busy": "2021-09-27T16:56:49.580116Z",
     "iopub.status.idle": "2021-09-27T16:56:49.583610Z",
     "shell.execute_reply": "2021-09-27T16:56:49.582942Z"
    },
    "papermill": {
     "duration": 0.042179,
     "end_time": "2021-09-27T16:56:49.583750",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.541571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train.columns[train.columns.str.contains('size')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c219ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.659723Z",
     "iopub.status.busy": "2021-09-27T16:56:49.658946Z",
     "iopub.status.idle": "2021-09-27T16:56:49.662170Z",
     "shell.execute_reply": "2021-09-27T16:56:49.661643Z"
    },
    "papermill": {
     "duration": 0.043311,
     "end_time": "2021-09-27T16:56:49.662315",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.619004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOBV = []\\nOBV.append(0)\\nfor i in range(1,train.shape[0]):\\n    for col in train.columns[train.columns.str.contains('_last')]:\\n        if train[col][i] > train[col][i-1]:\\n            OBV.append(OBV[-1]+train['trade_size_amin'])\\n        elif train[col][i] < train[col][i-1]:\\n            OBV.append(OBV[-1]-train['trade_size_amin'])\\n        else:\\n            OBV.append(OBV[-1])\\n            \\ntrain['OBV'] = OBV\\ntrain['OBV_EMA'] = train['OBV'].ewm(span=20).mean()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "OBV = []\n",
    "OBV.append(0)\n",
    "for i in range(1,train.shape[0]):\n",
    "    for col in train.columns[train.columns.str.contains('_last')]:\n",
    "        if train[col][i] > train[col][i-1]:\n",
    "            OBV.append(OBV[-1]+train['trade_size_amin'])\n",
    "        elif train[col][i] < train[col][i-1]:\n",
    "            OBV.append(OBV[-1]-train['trade_size_amin'])\n",
    "        else:\n",
    "            OBV.append(OBV[-1])\n",
    "            \n",
    "train['OBV'] = OBV\n",
    "train['OBV_EMA'] = train['OBV'].ewm(span=20).mean()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d07303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.739002Z",
     "iopub.status.busy": "2021-09-27T16:56:49.738101Z",
     "iopub.status.idle": "2021-09-27T16:56:49.742752Z",
     "shell.execute_reply": "2021-09-27T16:56:49.742149Z"
    },
    "papermill": {
     "duration": 0.045429,
     "end_time": "2021-09-27T16:56:49.742900",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.697471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5704a0bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:49.825705Z",
     "iopub.status.busy": "2021-09-27T16:56:49.825018Z",
     "iopub.status.idle": "2021-09-27T16:56:52.477025Z",
     "shell.execute_reply": "2021-09-27T16:56:52.477542Z"
    },
    "papermill": {
     "duration": 2.699451,
     "end_time": "2021-09-27T16:56:52.477732",
     "exception": false,
     "start_time": "2021-09-27T16:56:49.778281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518f35a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:52.559069Z",
     "iopub.status.busy": "2021-09-27T16:56:52.558387Z",
     "iopub.status.idle": "2021-09-27T16:56:52.766031Z",
     "shell.execute_reply": "2021-09-27T16:56:52.766554Z"
    },
    "papermill": {
     "duration": 0.25115,
     "end_time": "2021-09-27T16:56:52.766750",
     "exception": false,
     "start_time": "2021-09-27T16:56:52.515600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "463bf54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:56:52.844693Z",
     "iopub.status.busy": "2021-09-27T16:56:52.844015Z",
     "iopub.status.idle": "2021-09-27T16:57:03.237434Z",
     "shell.execute_reply": "2021-09-27T16:57:03.237931Z"
    },
    "papermill": {
     "duration": 10.433661,
     "end_time": "2021-09-27T16:57:03.238135",
     "exception": false,
     "start_time": "2021-09-27T16:56:52.804474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1',\n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1',]\n",
    "     #'relative_wap_balance1_sum_0c1',\n",
    "     #'relative_wap_balance1_sum_1c1',\n",
    "     #'relative_wap_balance1_sum_3c1',\n",
    "     #'relative_wap_balance1_sum_4c1',\n",
    "     #'relative_wap_balance1_sum_6c1',\n",
    "       \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ecc146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:57:03.345523Z",
     "iopub.status.busy": "2021-09-27T16:57:03.344849Z",
     "iopub.status.idle": "2021-09-27T16:57:03.466787Z",
     "shell.execute_reply": "2021-09-27T16:57:03.466193Z"
    },
    "papermill": {
     "duration": 0.178687,
     "end_time": "2021-09-27T16:57:03.466930",
     "exception": false,
     "start_time": "2021-09-27T16:57:03.288243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32fe653b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:57:03.548174Z",
     "iopub.status.busy": "2021-09-27T16:57:03.547528Z",
     "iopub.status.idle": "2021-09-27T16:57:03.550784Z",
     "shell.execute_reply": "2021-09-27T16:57:03.550256Z",
     "shell.execute_reply.started": "2021-09-27T15:42:06.842403Z"
    },
    "papermill": {
     "duration": 0.045861,
     "end_time": "2021-09-27T16:57:03.550951",
     "exception": false,
     "start_time": "2021-09-27T16:57:03.505090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso,  Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2adcd0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T16:57:03.645784Z",
     "iopub.status.busy": "2021-09-27T16:57:03.645082Z",
     "iopub.status.idle": "2021-09-27T17:10:34.805338Z",
     "shell.execute_reply": "2021-09-27T17:10:34.804684Z"
    },
    "papermill": {
     "duration": 811.216821,
     "end_time": "2021-09-27T17:10:34.805506",
     "exception": false,
     "start_time": "2021-09-27T16:57:03.588685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429012\ttraining's RMSPE: 0.198409\tvalid_1's rmse: 0.000440421\tvalid_1's RMSPE: 0.204418\n",
      "[500]\ttraining's rmse: 0.000406819\ttraining's RMSPE: 0.188145\tvalid_1's rmse: 0.000425328\tvalid_1's RMSPE: 0.197413\n",
      "[750]\ttraining's rmse: 0.000393476\ttraining's RMSPE: 0.181974\tvalid_1's rmse: 0.000418705\tvalid_1's RMSPE: 0.194339\n",
      "[1000]\ttraining's rmse: 0.000383557\ttraining's RMSPE: 0.177387\tvalid_1's rmse: 0.000414357\tvalid_1's RMSPE: 0.192321\n",
      "[1250]\ttraining's rmse: 0.000375413\ttraining's RMSPE: 0.173621\tvalid_1's rmse: 0.000411381\tvalid_1's RMSPE: 0.19094\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000371085\ttraining's RMSPE: 0.171619\tvalid_1's rmse: 0.00041005\tvalid_1's RMSPE: 0.190322\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042885\ttraining's RMSPE: 0.198681\tvalid_1's rmse: 0.000440384\tvalid_1's RMSPE: 0.202976\n",
      "[500]\ttraining's rmse: 0.00040692\ttraining's RMSPE: 0.188521\tvalid_1's rmse: 0.000425604\tvalid_1's RMSPE: 0.196164\n",
      "[750]\ttraining's rmse: 0.000393388\ttraining's RMSPE: 0.182252\tvalid_1's rmse: 0.00041841\tvalid_1's RMSPE: 0.192848\n",
      "[1000]\ttraining's rmse: 0.000383105\ttraining's RMSPE: 0.177488\tvalid_1's rmse: 0.000413876\tvalid_1's RMSPE: 0.190758\n",
      "[1250]\ttraining's rmse: 0.000374752\ttraining's RMSPE: 0.173618\tvalid_1's rmse: 0.000410689\tvalid_1's RMSPE: 0.189289\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000370539\ttraining's RMSPE: 0.171666\tvalid_1's rmse: 0.000409468\tvalid_1's RMSPE: 0.188727\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428827\ttraining's RMSPE: 0.198317\tvalid_1's rmse: 0.000466663\tvalid_1's RMSPE: 0.216629\n",
      "[500]\ttraining's rmse: 0.000406164\ttraining's RMSPE: 0.187836\tvalid_1's rmse: 0.000453873\tvalid_1's RMSPE: 0.210691\n",
      "[750]\ttraining's rmse: 0.000392677\ttraining's RMSPE: 0.181599\tvalid_1's rmse: 0.000447063\tvalid_1's RMSPE: 0.20753\n",
      "[1000]\ttraining's rmse: 0.000382567\ttraining's RMSPE: 0.176923\tvalid_1's rmse: 0.000443603\tvalid_1's RMSPE: 0.205924\n",
      "Early stopping, best iteration is:\n",
      "[1177]\ttraining's rmse: 0.000376859\ttraining's RMSPE: 0.174284\tvalid_1's rmse: 0.0004417\tvalid_1's RMSPE: 0.205041\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428678\ttraining's RMSPE: 0.198602\tvalid_1's rmse: 0.000446676\tvalid_1's RMSPE: 0.205874\n",
      "[500]\ttraining's rmse: 0.000405959\ttraining's RMSPE: 0.188076\tvalid_1's rmse: 0.000430782\tvalid_1's RMSPE: 0.198549\n",
      "[750]\ttraining's rmse: 0.000392916\ttraining's RMSPE: 0.182034\tvalid_1's rmse: 0.000423553\tvalid_1's RMSPE: 0.195217\n",
      "[1000]\ttraining's rmse: 0.000382771\ttraining's RMSPE: 0.177334\tvalid_1's rmse: 0.000418934\tvalid_1's RMSPE: 0.193088\n",
      "[1250]\ttraining's rmse: 0.000374608\ttraining's RMSPE: 0.173552\tvalid_1's rmse: 0.000415968\tvalid_1's RMSPE: 0.191721\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.00037047\ttraining's RMSPE: 0.171634\tvalid_1's rmse: 0.000414937\tvalid_1's RMSPE: 0.191246\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429355\ttraining's RMSPE: 0.198595\tvalid_1's rmse: 0.000443319\tvalid_1's RMSPE: 0.205651\n",
      "[500]\ttraining's rmse: 0.000407532\ttraining's RMSPE: 0.188501\tvalid_1's rmse: 0.000429753\tvalid_1's RMSPE: 0.199357\n",
      "[750]\ttraining's rmse: 0.00039417\ttraining's RMSPE: 0.182321\tvalid_1's rmse: 0.000422975\tvalid_1's RMSPE: 0.196213\n",
      "[1000]\ttraining's rmse: 0.000383876\ttraining's RMSPE: 0.177559\tvalid_1's rmse: 0.000419068\tvalid_1's RMSPE: 0.194401\n",
      "[1250]\ttraining's rmse: 0.000375716\ttraining's RMSPE: 0.173785\tvalid_1's rmse: 0.000416363\tvalid_1's RMSPE: 0.193146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000371228\ttraining's RMSPE: 0.171709\tvalid_1's rmse: 0.000415059\tvalid_1's RMSPE: 0.192541\n",
      "Our out of folds RMSPE is 0.19366412769825608\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1400,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb1= train_and_evaluate_lgb(train, test,params0)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fcfd284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:10:34.931932Z",
     "iopub.status.busy": "2021-09-27T17:10:34.931187Z",
     "iopub.status.idle": "2021-09-27T17:24:42.746714Z",
     "shell.execute_reply": "2021-09-27T17:24:42.747272Z"
    },
    "papermill": {
     "duration": 847.889393,
     "end_time": "2021-09-27T17:24:42.747673",
     "exception": false,
     "start_time": "2021-09-27T17:10:34.858280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428118\ttraining's RMSPE: 0.19821\tvalid_1's rmse: 0.000446478\tvalid_1's RMSPE: 0.206339\n",
      "[500]\ttraining's rmse: 0.000406289\ttraining's RMSPE: 0.188103\tvalid_1's rmse: 0.000432274\tvalid_1's RMSPE: 0.199774\n",
      "[750]\ttraining's rmse: 0.000392293\ttraining's RMSPE: 0.181624\tvalid_1's rmse: 0.000424188\tvalid_1's RMSPE: 0.196037\n",
      "[1000]\ttraining's rmse: 0.000382443\ttraining's RMSPE: 0.177063\tvalid_1's rmse: 0.000420078\tvalid_1's RMSPE: 0.194138\n",
      "[1250]\ttraining's rmse: 0.000374347\ttraining's RMSPE: 0.173315\tvalid_1's rmse: 0.000417374\tvalid_1's RMSPE: 0.192888\n",
      "[1500]\ttraining's rmse: 0.000367493\ttraining's RMSPE: 0.170141\tvalid_1's rmse: 0.000415488\tvalid_1's RMSPE: 0.192017\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's rmse: 0.000367493\ttraining's RMSPE: 0.170141\tvalid_1's rmse: 0.000415488\tvalid_1's RMSPE: 0.192017\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429083\ttraining's RMSPE: 0.19845\tvalid_1's rmse: 0.000463547\tvalid_1's RMSPE: 0.215118\n",
      "[500]\ttraining's rmse: 0.000406999\ttraining's RMSPE: 0.188236\tvalid_1's rmse: 0.000449197\tvalid_1's RMSPE: 0.208458\n",
      "[750]\ttraining's rmse: 0.000393475\ttraining's RMSPE: 0.181981\tvalid_1's rmse: 0.000440826\tvalid_1's RMSPE: 0.204573\n",
      "[1000]\ttraining's rmse: 0.00038354\ttraining's RMSPE: 0.177387\tvalid_1's rmse: 0.000436845\tvalid_1's RMSPE: 0.202726\n",
      "Early stopping, best iteration is:\n",
      "[1019]\ttraining's rmse: 0.000382744\ttraining's RMSPE: 0.177018\tvalid_1's rmse: 0.000436343\tvalid_1's RMSPE: 0.202493\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428315\ttraining's RMSPE: 0.19832\tvalid_1's rmse: 0.000440452\tvalid_1's RMSPE: 0.203475\n",
      "[500]\ttraining's rmse: 0.000406313\ttraining's RMSPE: 0.188132\tvalid_1's rmse: 0.000425598\tvalid_1's RMSPE: 0.196613\n",
      "[750]\ttraining's rmse: 0.000393067\ttraining's RMSPE: 0.181999\tvalid_1's rmse: 0.000418335\tvalid_1's RMSPE: 0.193258\n",
      "[1000]\ttraining's rmse: 0.000382797\ttraining's RMSPE: 0.177244\tvalid_1's rmse: 0.000413703\tvalid_1's RMSPE: 0.191118\n",
      "[1250]\ttraining's rmse: 0.000374561\ttraining's RMSPE: 0.173431\tvalid_1's rmse: 0.00041038\tvalid_1's RMSPE: 0.189583\n",
      "[1500]\ttraining's rmse: 0.00036762\ttraining's RMSPE: 0.170216\tvalid_1's rmse: 0.000408408\tvalid_1's RMSPE: 0.188672\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's rmse: 0.00036762\ttraining's RMSPE: 0.170216\tvalid_1's rmse: 0.000408408\tvalid_1's RMSPE: 0.188672\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429013\ttraining's RMSPE: 0.198303\tvalid_1's rmse: 0.000444515\tvalid_1's RMSPE: 0.206757\n",
      "[500]\ttraining's rmse: 0.000406296\ttraining's RMSPE: 0.187803\tvalid_1's rmse: 0.000430321\tvalid_1's RMSPE: 0.200155\n",
      "[750]\ttraining's rmse: 0.000392792\ttraining's RMSPE: 0.181561\tvalid_1's rmse: 0.000423413\tvalid_1's RMSPE: 0.196942\n",
      "[1000]\ttraining's rmse: 0.000382827\ttraining's RMSPE: 0.176955\tvalid_1's rmse: 0.00041961\tvalid_1's RMSPE: 0.195173\n",
      "[1250]\ttraining's rmse: 0.000374493\ttraining's RMSPE: 0.173103\tvalid_1's rmse: 0.000417257\tvalid_1's RMSPE: 0.194079\n",
      "Early stopping, best iteration is:\n",
      "[1353]\ttraining's rmse: 0.000371573\ttraining's RMSPE: 0.171753\tvalid_1's rmse: 0.000416548\tvalid_1's RMSPE: 0.193749\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000427716\ttraining's RMSPE: 0.198174\tvalid_1's rmse: 0.000447623\tvalid_1's RMSPE: 0.206236\n",
      "[500]\ttraining's rmse: 0.000405659\ttraining's RMSPE: 0.187954\tvalid_1's rmse: 0.000434453\tvalid_1's RMSPE: 0.200168\n",
      "[750]\ttraining's rmse: 0.000391862\ttraining's RMSPE: 0.181561\tvalid_1's rmse: 0.000427917\tvalid_1's RMSPE: 0.197157\n",
      "[1000]\ttraining's rmse: 0.000381764\ttraining's RMSPE: 0.176883\tvalid_1's rmse: 0.000424188\tvalid_1's RMSPE: 0.195438\n",
      "[1250]\ttraining's rmse: 0.000373803\ttraining's RMSPE: 0.173194\tvalid_1's rmse: 0.000422027\tvalid_1's RMSPE: 0.194443\n",
      "[1500]\ttraining's rmse: 0.000366982\ttraining's RMSPE: 0.170034\tvalid_1's rmse: 0.000420519\tvalid_1's RMSPE: 0.193748\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's rmse: 0.000366982\ttraining's RMSPE: 0.170034\tvalid_1's rmse: 0.000420519\tvalid_1's RMSPE: 0.193748\n",
      "Our out of folds RMSPE is 0.19418947073621265\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=1111\n",
    "params111 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 1111, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1500,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb2= train_and_evaluate_lgb(train, test,params111)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "086c34aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:24:42.889860Z",
     "iopub.status.busy": "2021-09-27T17:24:42.888921Z",
     "iopub.status.idle": "2021-09-27T17:24:42.892642Z",
     "shell.execute_reply": "2021-09-27T17:24:42.893133Z"
    },
    "papermill": {
     "duration": 0.07897,
     "end_time": "2021-09-27T17:24:42.893306",
     "exception": false,
     "start_time": "2021-09-27T17:24:42.814336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00187833, 0.0020438 , 0.0020438 ]),\n",
       " array([0.001803  , 0.00201663, 0.00201663]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb1,predictions_lgb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9603d8ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T17:24:43.033116Z",
     "iopub.status.busy": "2021-09-27T17:24:43.032053Z",
     "iopub.status.idle": "2021-09-27T19:12:26.001795Z",
     "shell.execute_reply": "2021-09-27T19:12:26.002666Z",
     "shell.execute_reply.started": "2021-09-21T09:33:10.301784Z"
    },
    "papermill": {
     "duration": 6463.042053,
     "end_time": "2021-09-27T19:12:26.002983",
     "exception": false,
     "start_time": "2021-09-27T17:24:42.960930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0011699\ttest: 0.0035249\tbest: 0.0035249 (0)\ttotal: 992ms\tremaining: 24m 46s\n",
      "250:\tlearn: 0.0005299\ttest: 0.0016338\tbest: 0.0016337 (249)\ttotal: 3m 17s\tremaining: 16m 22s\n",
      "500:\tlearn: 0.0004497\ttest: 0.0011939\tbest: 0.0011939 (500)\ttotal: 6m 52s\tremaining: 13m 42s\n",
      "750:\tlearn: 0.0004218\ttest: 0.0011289\tbest: 0.0011289 (750)\ttotal: 10m 31s\tremaining: 10m 29s\n",
      "1000:\tlearn: 0.0004066\ttest: 0.0010971\tbest: 0.0010971 (1000)\ttotal: 14m 8s\tremaining: 7m 2s\n",
      "1250:\tlearn: 0.0003960\ttest: 0.0010772\tbest: 0.0010772 (1250)\ttotal: 17m 43s\tremaining: 3m 31s\n",
      "1499:\tlearn: 0.0003881\ttest: 0.0010640\tbest: 0.0010640 (1499)\ttotal: 21m 17s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.00106397878\n",
      "bestIteration = 1499\n",
      "\n",
      "\n",
      "Seed-11 | Fold-0 | OOF Score: 0.19740093543553766\n",
      "\n",
      "0:\tlearn: 0.0011686\ttest: 0.0035331\tbest: 0.0035331 (0)\ttotal: 828ms\tremaining: 20m 40s\n",
      "250:\tlearn: 0.0005233\ttest: 0.0016035\tbest: 0.0016035 (250)\ttotal: 3m 16s\tremaining: 16m 18s\n",
      "500:\tlearn: 0.0004493\ttest: 0.0012159\tbest: 0.0012159 (500)\ttotal: 6m 51s\tremaining: 13m 40s\n",
      "750:\tlearn: 0.0004213\ttest: 0.0011455\tbest: 0.0011455 (750)\ttotal: 10m 31s\tremaining: 10m 29s\n",
      "1000:\tlearn: 0.0004065\ttest: 0.0011138\tbest: 0.0011138 (1000)\ttotal: 14m 8s\tremaining: 7m 3s\n",
      "1250:\tlearn: 0.0003962\ttest: 0.0010945\tbest: 0.0010945 (1250)\ttotal: 17m 45s\tremaining: 3m 32s\n",
      "1499:\tlearn: 0.0003885\ttest: 0.0010806\tbest: 0.0010806 (1499)\ttotal: 21m 19s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.001080628185\n",
      "bestIteration = 1499\n",
      "\n",
      "\n",
      "Seed-11 | Fold-1 | OOF Score: 0.19906085639741888\n",
      "\n",
      "0:\tlearn: 0.0011735\ttest: 0.0035481\tbest: 0.0035481 (0)\ttotal: 897ms\tremaining: 22m 25s\n",
      "250:\tlearn: 0.0005216\ttest: 0.0016079\tbest: 0.0016079 (250)\ttotal: 3m 15s\tremaining: 16m 11s\n",
      "500:\tlearn: 0.0004493\ttest: 0.0012316\tbest: 0.0012316 (500)\ttotal: 6m 51s\tremaining: 13m 40s\n",
      "750:\tlearn: 0.0004222\ttest: 0.0011679\tbest: 0.0011679 (750)\ttotal: 10m 31s\tremaining: 10m 29s\n",
      "1000:\tlearn: 0.0004071\ttest: 0.0011391\tbest: 0.0011390 (999)\ttotal: 14m 10s\tremaining: 7m 3s\n",
      "1250:\tlearn: 0.0003968\ttest: 0.0011218\tbest: 0.0011218 (1250)\ttotal: 17m 47s\tremaining: 3m 32s\n",
      "1499:\tlearn: 0.0003891\ttest: 0.0011096\tbest: 0.0011096 (1498)\ttotal: 21m 23s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.001109611279\n",
      "bestIteration = 1498\n",
      "\n",
      "Shrink model to first 1499 iterations.\n",
      "\n",
      "Seed-11 | Fold-2 | OOF Score: 0.1955249713095949\n",
      "\n",
      "0:\tlearn: 0.0011766\ttest: 0.0035474\tbest: 0.0035474 (0)\ttotal: 806ms\tremaining: 20m 7s\n",
      "250:\tlearn: 0.0005176\ttest: 0.0015762\tbest: 0.0015762 (250)\ttotal: 3m 16s\tremaining: 16m 18s\n",
      "500:\tlearn: 0.0004484\ttest: 0.0012181\tbest: 0.0012181 (500)\ttotal: 6m 52s\tremaining: 13m 43s\n",
      "750:\tlearn: 0.0004212\ttest: 0.0011508\tbest: 0.0011508 (750)\ttotal: 10m 34s\tremaining: 10m 32s\n",
      "1000:\tlearn: 0.0004061\ttest: 0.0011202\tbest: 0.0011202 (1000)\ttotal: 14m 13s\tremaining: 7m 5s\n",
      "1250:\tlearn: 0.0003960\ttest: 0.0011018\tbest: 0.0011018 (1250)\ttotal: 17m 52s\tremaining: 3m 33s\n",
      "1499:\tlearn: 0.0003880\ttest: 0.0010877\tbest: 0.0010877 (1498)\ttotal: 21m 29s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.001087747098\n",
      "bestIteration = 1498\n",
      "\n",
      "Shrink model to first 1499 iterations.\n",
      "\n",
      "Seed-11 | Fold-3 | OOF Score: 0.1986372293916728\n",
      "\n",
      "0:\tlearn: 0.0011755\ttest: 0.0035351\tbest: 0.0035351 (0)\ttotal: 775ms\tremaining: 19m 21s\n",
      "250:\tlearn: 0.0005256\ttest: 0.0016193\tbest: 0.0016193 (250)\ttotal: 3m 19s\tremaining: 16m 34s\n",
      "500:\tlearn: 0.0004513\ttest: 0.0012025\tbest: 0.0012025 (500)\ttotal: 6m 56s\tremaining: 13m 50s\n",
      "750:\tlearn: 0.0004230\ttest: 0.0011323\tbest: 0.0011323 (750)\ttotal: 10m 37s\tremaining: 10m 35s\n",
      "1000:\tlearn: 0.0004077\ttest: 0.0010994\tbest: 0.0010994 (1000)\ttotal: 14m 17s\tremaining: 7m 7s\n",
      "1250:\tlearn: 0.0003973\ttest: 0.0010795\tbest: 0.0010795 (1250)\ttotal: 17m 57s\tremaining: 3m 34s\n",
      "1499:\tlearn: 0.0003895\ttest: 0.0010662\tbest: 0.0010662 (1499)\ttotal: 21m 34s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.001066197837\n",
      "bestIteration = 1499\n",
      "\n",
      "\n",
      "Seed-11 | Fold-4 | OOF Score: 0.19975596445952373\n",
      "\n",
      "\n",
      "Seed: 11 | Aggregate OOF Score: 0.1980759913987496\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.1980759913987496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_cat(train, test):\n",
    "    FOLD = 5\n",
    "    SEEDS = [11]\n",
    "\n",
    "    fet_imp = 0\n",
    "    counter = 0\n",
    "    oof_score = 0\n",
    "    y_pred_meta_cb = np.zeros((train.shape[0], 1))\n",
    "    y_pred_final_cb = 0\n",
    "    cat_cols_indices = [train.columns.get_loc('stock_id')]\n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    \n",
    "    Xtrain = train[features].copy()\n",
    "    Ytrain = train['target'].copy()\n",
    "    Xtest = test[features].copy()\n",
    "\n",
    "\n",
    "    for sidx, seed in enumerate(SEEDS):\n",
    "        seed_score = 0\n",
    "    \n",
    "        kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "        for idx, (vtrain, val) in enumerate(kfold.split(train)):\n",
    "            counter += 1\n",
    "        \n",
    "            train_x, train_y = Xtrain.iloc[vtrain], Ytrain.iloc[vtrain]\n",
    "            val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n",
    "            sample_weight = 1/np.square(train_y)\n",
    "        \n",
    "            model = CatBoostRegressor(\n",
    "                objective='RMSE',\n",
    "                eval_metric='RMSE',\n",
    "                num_boost_round=1500,\n",
    "                #max_ctr_complexity=15,\n",
    "                #od_wait=500, \n",
    "                od_type='Iter',\n",
    "                learning_rate=0.05, # 0.05\n",
    "                #reg_lambda=0.05,\n",
    "                bootstrap_type='Bernoulli',\n",
    "                use_best_model=True, \n",
    "                #subsample=0.8,\n",
    "                grow_policy='Lossguide',\n",
    "                #min_data_in_leaf=500, \n",
    "                #task_type='GPU',\n",
    "                random_state=21 # 0\n",
    "                )\n",
    "\n",
    "            model.fit(train_x, train_y, eval_set=[(val_x, val_y)], \n",
    "                    cat_features=cat_cols_indices, \n",
    "                    sample_weight=sample_weight, \n",
    "                    early_stopping_rounds=100, verbose=250)\n",
    "\n",
    "            y_pred = model.predict(val_x)\n",
    "            y_pred_meta_cb[val] += np.array([y_pred]).T\n",
    "            y_pred_final_cb += model.predict(Xtest)\n",
    "        \n",
    "            score = rmspe(val_y, y_pred)\n",
    "            oof_score += score\n",
    "            seed_score += score\n",
    "            fet_imp += model.feature_importances_\n",
    "            print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n",
    "        \n",
    "            #joblib.dump(model, f'./cb_model_{idx + 1}C.txt')\n",
    "    \n",
    "        print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "    y_pred_meta_cb = y_pred_meta_cb / float(len(SEEDS))\n",
    "    y_pred_final_cb = y_pred_final_cb / float(counter)\n",
    "    fet_imp = fet_imp / float(counter)\n",
    "    oof_score /= float(counter)\n",
    "    print(\"Aggregate OOF Score: {}\".format(oof_score))\n",
    "\n",
    "    del Xtrain, Ytrain, Xtest\n",
    "    gc.collect()\n",
    "    return y_pred_final_cb\n",
    "# Traing and evaluate\n",
    "predictions_cat = train_and_evaluate_cat(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91198c77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:26.168120Z",
     "iopub.status.busy": "2021-09-27T19:12:26.167459Z",
     "iopub.status.idle": "2021-09-27T19:12:26.172653Z",
     "shell.execute_reply": "2021-09-27T19:12:26.173160Z",
     "shell.execute_reply.started": "2021-09-21T11:37:32.810494Z"
    },
    "papermill": {
     "duration": 0.089643,
     "end_time": "2021-09-27T19:12:26.173384",
     "exception": false,
     "start_time": "2021-09-27T19:12:26.083741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00419398, 0.0047687 , 0.0047687 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a08e8a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:26.339686Z",
     "iopub.status.busy": "2021-09-27T19:12:26.338699Z",
     "iopub.status.idle": "2021-09-27T19:12:26.343251Z",
     "shell.execute_reply": "2021-09-27T19:12:26.342640Z"
    },
    "papermill": {
     "duration": 0.091196,
     "end_time": "2021-09-27T19:12:26.343399",
     "exception": false,
     "start_time": "2021-09-27T19:12:26.252203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_and_evaluate_lgb(train, test):\\n    FOLD = 5\\n    SEEDS = [1111]\\n\\n    fet_imp = 0\\n    counter = 0\\n    oof_score = 0\\n    y_pred_meta_lgb = np.zeros((train.shape[0], 1))\\n    y_pred_final_lgb = 0\\n    cat_cols_indices = [train.columns.get_loc(\\'stock_id\\')]\\n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\\n    \\n    Xtrain = train[features].copy()\\n    Ytrain = train[\\'target\\'].copy()\\n    Xtest = test[features].copy()\\n\\n\\n    for sidx, seed in enumerate(SEEDS):\\n        seed_score = 0\\n    \\n        kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\\n\\n        for idx, (vtrain, val) in enumerate(kfold.split(train)):\\n            counter += 1\\n        \\n            train_x, train_y = Xtrain.iloc[vtrain], Ytrain.iloc[vtrain]\\n            val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\\n            sample_weight = 1/np.square(train_y)\\n        \\n            model = LGBMRegressor(\\n            boosting_type=\\'gbdt\\', \\n            num_leaves=175, \\n            max_depth=9, \\n            learning_rate=0.05, \\n            n_estimators=1500,\\n            objective=\\'regression\\', \\n            #importance_type=\\'gain\\',\\n            #device=\\'gpu\\',\\n            #max_bin=100,\\n            #min_child_samples=500, \\n            subsample=0.85, \\n            subsample_freq=5, \\n            colsample_bytree=0.55, \\n            reg_lambda=0.05,\\n            random_state=1111,\\n            bagging_seed=0,\\n            feature_fraction_seed=0\\n        )\\n        \\n            model.fit(train_x, train_y, eval_metric=\\'rmse\\',\\n                    eval_set=[(train_x, train_y), (val_x, val_y)],\\n                      early_stopping_rounds=100, verbose=500,\\n                      categorical_feature=cat_cols_indices,\\n                      sample_weight=sample_weight)\\n\\n            y_pred = model.predict(val_x, num_iteration=model.best_iteration_)\\n            y_pred_meta_lgb[val] += np.array([y_pred]).T\\n            y_pred_final_lgb += model.predict(Xtest, num_iteration=model.best_iteration_)\\n        \\n            score = rmspe(val_y, y_pred)\\n            oof_score += score\\n            seed_score += score\\n            fet_imp += model.feature_importances_\\n            print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\\n        \\n            joblib.dump(model, f\\'./lgb_model_{idx + 1}C.txt\\')\\n    \\n        print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\\n\\n\\n    y_pred_meta_lgb = y_pred_meta_lgb / float(len(SEEDS))\\n    y_pred_final_lgb = y_pred_final_lgb / float(counter)\\n    fet_imp = fet_imp / float(counter)\\n    oof_score /= float(counter)\\n    print(\"Aggregate OOF Score: {}\".format(oof_score))\\n    del Xtrain, Ytrain, Xtest\\n    gc.collect()\\n    return y_pred_final_lgb\\n    \\npredictions_lgb = train_and_evaluate_lgb(train, test)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def train_and_evaluate_lgb(train, test):\n",
    "    FOLD = 5\n",
    "    SEEDS = [1111]\n",
    "\n",
    "    fet_imp = 0\n",
    "    counter = 0\n",
    "    oof_score = 0\n",
    "    y_pred_meta_lgb = np.zeros((train.shape[0], 1))\n",
    "    y_pred_final_lgb = 0\n",
    "    cat_cols_indices = [train.columns.get_loc('stock_id')]\n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    \n",
    "    Xtrain = train[features].copy()\n",
    "    Ytrain = train['target'].copy()\n",
    "    Xtest = test[features].copy()\n",
    "\n",
    "\n",
    "    for sidx, seed in enumerate(SEEDS):\n",
    "        seed_score = 0\n",
    "    \n",
    "        kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "        for idx, (vtrain, val) in enumerate(kfold.split(train)):\n",
    "            counter += 1\n",
    "        \n",
    "            train_x, train_y = Xtrain.iloc[vtrain], Ytrain.iloc[vtrain]\n",
    "            val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n",
    "            sample_weight = 1/np.square(train_y)\n",
    "        \n",
    "            model = LGBMRegressor(\n",
    "            boosting_type='gbdt', \n",
    "            num_leaves=175, \n",
    "            max_depth=9, \n",
    "            learning_rate=0.05, \n",
    "            n_estimators=1500,\n",
    "            objective='regression', \n",
    "            #importance_type='gain',\n",
    "            #device='gpu',\n",
    "            #max_bin=100,\n",
    "            #min_child_samples=500, \n",
    "            subsample=0.85, \n",
    "            subsample_freq=5, \n",
    "            colsample_bytree=0.55, \n",
    "            reg_lambda=0.05,\n",
    "            random_state=1111,\n",
    "            bagging_seed=0,\n",
    "            feature_fraction_seed=0\n",
    "        )\n",
    "        \n",
    "            model.fit(train_x, train_y, eval_metric='rmse',\n",
    "                    eval_set=[(train_x, train_y), (val_x, val_y)],\n",
    "                      early_stopping_rounds=100, verbose=500,\n",
    "                      categorical_feature=cat_cols_indices,\n",
    "                      sample_weight=sample_weight)\n",
    "\n",
    "            y_pred = model.predict(val_x, num_iteration=model.best_iteration_)\n",
    "            y_pred_meta_lgb[val] += np.array([y_pred]).T\n",
    "            y_pred_final_lgb += model.predict(Xtest, num_iteration=model.best_iteration_)\n",
    "        \n",
    "            score = rmspe(val_y, y_pred)\n",
    "            oof_score += score\n",
    "            seed_score += score\n",
    "            fet_imp += model.feature_importances_\n",
    "            print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n",
    "        \n",
    "            joblib.dump(model, f'./lgb_model_{idx + 1}C.txt')\n",
    "    \n",
    "        print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "    y_pred_meta_lgb = y_pred_meta_lgb / float(len(SEEDS))\n",
    "    y_pred_final_lgb = y_pred_final_lgb / float(counter)\n",
    "    fet_imp = fet_imp / float(counter)\n",
    "    oof_score /= float(counter)\n",
    "    print(\"Aggregate OOF Score: {}\".format(oof_score))\n",
    "    del Xtrain, Ytrain, Xtest\n",
    "    gc.collect()\n",
    "    return y_pred_final_lgb\n",
    "    \n",
    "predictions_lgb = train_and_evaluate_lgb(train, test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b4f4daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:26.507900Z",
     "iopub.status.busy": "2021-09-27T19:12:26.506904Z",
     "iopub.status.idle": "2021-09-27T19:12:26.510193Z",
     "shell.execute_reply": "2021-09-27T19:12:26.509634Z"
    },
    "papermill": {
     "duration": 0.087167,
     "end_time": "2021-09-27T19:12:26.510333",
     "exception": false,
     "start_time": "2021-09-27T19:12:26.423166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predictions_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd4a2fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:26.680961Z",
     "iopub.status.busy": "2021-09-27T19:12:26.680225Z",
     "iopub.status.idle": "2021-09-27T19:12:32.341835Z",
     "shell.execute_reply": "2021-09-27T19:12:32.342371Z"
    },
    "papermill": {
     "duration": 5.750934,
     "end_time": "2021-09-27T19:12:32.342577",
     "exception": false,
     "start_time": "2021-09-27T19:12:26.591643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4be06d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:32.506599Z",
     "iopub.status.busy": "2021-09-27T19:12:32.505883Z",
     "iopub.status.idle": "2021-09-27T19:12:44.439276Z",
     "shell.execute_reply": "2021-09-27T19:12:44.438703Z"
    },
    "papermill": {
     "duration": 12.017151,
     "end_time": "2021-09-27T19:12:44.439463",
     "exception": false,
     "start_time": "2021-09-27T19:12:32.422312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e42a16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:12:44.608005Z",
     "iopub.status.busy": "2021-09-27T19:12:44.607153Z",
     "iopub.status.idle": "2021-09-27T19:13:20.109480Z",
     "shell.execute_reply": "2021-09-27T19:13:20.108856Z"
    },
    "papermill": {
     "duration": 35.590246,
     "end_time": "2021-09-27T19:13:20.109632",
     "exception": false,
     "start_time": "2021-09-27T19:12:44.519386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=1111,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e1b057a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:20.278937Z",
     "iopub.status.busy": "2021-09-27T19:13:20.278263Z",
     "iopub.status.idle": "2021-09-27T19:13:20.287629Z",
     "shell.execute_reply": "2021-09-27T19:13:20.287088Z"
    },
    "papermill": {
     "duration": 0.098594,
     "end_time": "2021-09-27T19:13:20.287785",
     "exception": false,
     "start_time": "2021-09-27T19:13:20.189191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15eef84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:20.464031Z",
     "iopub.status.busy": "2021-09-27T19:13:20.460563Z",
     "iopub.status.idle": "2021-09-27T19:13:22.548872Z",
     "shell.execute_reply": "2021-09-27T19:13:22.548259Z"
    },
    "papermill": {
     "duration": 2.181789,
     "end_time": "2021-09-27T19:13:22.549015",
     "exception": false,
     "start_time": "2021-09-27T19:13:20.367226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()#method = 'pearson' 'kendall' 'spearman'\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ba24463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:22.719923Z",
     "iopub.status.busy": "2021-09-27T19:13:22.718897Z",
     "iopub.status.idle": "2021-09-27T19:13:22.722260Z",
     "shell.execute_reply": "2021-09-27T19:13:22.721733Z"
    },
    "papermill": {
     "duration": 0.092273,
     "end_time": "2021-09-27T19:13:22.722400",
     "exception": false,
     "start_time": "2021-09-27T19:13:22.630127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1',]\n",
    "     #'relative_wap_balance1_sum_0c1',\n",
    "     #'relative_wap_balance1_sum_1c1',\n",
    "     #'relative_wap_balance1_sum_3c1',\n",
    "     #'relative_wap_balance1_sum_4c1',\n",
    "     #'relative_wap_balance1_sum_6c1',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c3a0ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:22.890086Z",
     "iopub.status.busy": "2021-09-27T19:13:22.889389Z",
     "iopub.status.idle": "2021-09-27T19:13:23.058854Z",
     "shell.execute_reply": "2021-09-27T19:13:23.058256Z"
    },
    "papermill": {
     "duration": 0.253854,
     "end_time": "2021-09-27T19:13:23.058995",
     "exception": false,
     "start_time": "2021-09-27T19:13:22.805141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31782b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:23.233644Z",
     "iopub.status.busy": "2021-09-27T19:13:23.232993Z",
     "iopub.status.idle": "2021-09-27T19:13:29.520243Z",
     "shell.execute_reply": "2021-09-27T19:13:29.520771Z"
    },
    "papermill": {
     "duration": 6.37928,
     "end_time": "2021-09-27T19:13:29.520952",
     "exception": false,
     "start_time": "2021-09-27T19:13:23.141672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "train1=train_nn\n",
    "test1=test_nn\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "567917f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:29.697907Z",
     "iopub.status.busy": "2021-09-27T19:13:29.695982Z",
     "iopub.status.idle": "2021-09-27T19:13:29.715052Z",
     "shell.execute_reply": "2021-09-27T19:13:29.714365Z"
    },
    "papermill": {
     "duration": 0.110544,
     "end_time": "2021-09-27T19:13:29.715196",
     "exception": false,
     "start_time": "2021-09-27T19:13:29.604652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(284,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "534435f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:29.888443Z",
     "iopub.status.busy": "2021-09-27T19:13:29.887409Z",
     "iopub.status.idle": "2021-09-27T19:13:29.893929Z",
     "shell.execute_reply": "2021-09-27T19:13:29.893273Z"
    },
    "papermill": {
     "duration": 0.094207,
     "end_time": "2021-09-27T19:13:29.894076",
     "exception": false,
     "start_time": "2021-09-27T19:13:29.799869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f71fa12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:13:30.083555Z",
     "iopub.status.busy": "2021-09-27T19:13:30.074392Z",
     "iopub.status.idle": "2021-09-27T19:28:32.182170Z",
     "shell.execute_reply": "2021-09-27T19:28:32.182889Z"
    },
    "papermill": {
     "duration": 902.206082,
     "end_time": "2021-09-27T19:28:32.183155",
     "exception": false,
     "start_time": "2021-09-27T19:13:29.977073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 19ms/step - loss: 24.6384 - val_loss: 1.2070\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8579 - val_loss: 0.5029\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6502 - val_loss: 0.5070\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6272 - val_loss: 0.7169\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6451 - val_loss: 0.7006\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5770 - val_loss: 0.4881\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5742 - val_loss: 0.4307\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5758 - val_loss: 0.4771\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3314 - val_loss: 0.2645\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2742 - val_loss: 0.2714\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2808 - val_loss: 0.2289\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2682 - val_loss: 0.2205\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2620 - val_loss: 0.2923\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2886 - val_loss: 8.2752\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 3.4414 - val_loss: 0.2451\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2459 - val_loss: 0.2277\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2343 - val_loss: 0.2190\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2293 - val_loss: 0.2742\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2294 - val_loss: 0.2156\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2221 - val_loss: 0.2140\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2237 - val_loss: 0.2259\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2491 - val_loss: 0.2165\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2845 - val_loss: 0.3119\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3392 - val_loss: 0.2215\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2311 - val_loss: 0.2159\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2254 - val_loss: 0.2161\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2284 - val_loss: 0.2378\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2131 - val_loss: 0.2099\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2098 - val_loss: 0.2110\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2105\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2097 - val_loss: 0.2110\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2090 - val_loss: 0.2108\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2092 - val_loss: 0.2096\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2106\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2131\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2092 - val_loss: 0.2102\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2082 - val_loss: 0.2103\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2089 - val_loss: 0.2103\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2086 - val_loss: 0.2101\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2115 - val_loss: 0.2111\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2089\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2091\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2064 - val_loss: 0.2095\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2067 - val_loss: 0.2091\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2086\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2069 - val_loss: 0.2085\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2092\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2058 - val_loss: 0.2091\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2056 - val_loss: 0.2104\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2068 - val_loss: 0.2092\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2064 - val_loss: 0.2092\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2107\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2053 - val_loss: 0.2083\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2046 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2085\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2086\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2085\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2087\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2046 - val_loss: 0.2087\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2085\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2082\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2082\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2083\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2083\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2081\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2083\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2032 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2082\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2045 - val_loss: 0.2082\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2082\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2082\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 92/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 93/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2081\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 95/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2082\n",
      "Epoch 96/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2032 - val_loss: 0.2082\n",
      "Fold 1 NN: 0.20812\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 25.5336 - val_loss: 0.5264\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5506 - val_loss: 1.2809\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.9044 - val_loss: 0.9032\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.9480 - val_loss: 0.7436\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0073 - val_loss: 0.7755\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.8892 - val_loss: 0.4758\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.3276 - val_loss: 0.2860\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4654 - val_loss: 9.2211\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 2.4387 - val_loss: 0.5133\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4690 - val_loss: 0.4286\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4776 - val_loss: 0.5387\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4755 - val_loss: 0.4371\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4297 - val_loss: 0.5103\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4216 - val_loss: 0.3545\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2359 - val_loss: 0.2221\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2133 - val_loss: 0.2218\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2125 - val_loss: 0.2233\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2197\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2198\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2111 - val_loss: 0.2198\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2113 - val_loss: 0.2194\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2101 - val_loss: 0.2192\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2106 - val_loss: 0.2216\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2190\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2094 - val_loss: 0.2189\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2114 - val_loss: 0.2172\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2112 - val_loss: 0.2207\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2211 - val_loss: 0.2181\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2173\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2097 - val_loss: 0.2258\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2121 - val_loss: 0.2171\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2196\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2134 - val_loss: 0.2352\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2073 - val_loss: 0.2153\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2041 - val_loss: 0.2145\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2142\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2147\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2046 - val_loss: 0.2149\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2144\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2164\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2149\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2154\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2022 - val_loss: 0.2140\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2133\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2026 - val_loss: 0.2139\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2131\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2135\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2022 - val_loss: 0.2136\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2135\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2129\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2130\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2028 - val_loss: 0.2138\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2136\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2143\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2022 - val_loss: 0.2136\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2018 - val_loss: 0.2141\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2017 - val_loss: 0.2135\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2138\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2143\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2142\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2013 - val_loss: 0.2143\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2139\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2138\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2143\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2133\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2013 - val_loss: 0.2139\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2136\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2135\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2137\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2135\n",
      "Fold 2 NN: 0.21291\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 27.2659 - val_loss: 1.2580\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.8415 - val_loss: 0.6550\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 4s 23ms/step - loss: 0.6453 - val_loss: 0.5600\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.6341 - val_loss: 0.6786\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.5279 - val_loss: 0.2461\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3043 - val_loss: 0.3381\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3051 - val_loss: 0.3480\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.0800 - val_loss: 0.4731\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3450 - val_loss: 0.4195\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.3081 - val_loss: 0.2283\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2514 - val_loss: 0.2906\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2905 - val_loss: 0.4550\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4470 - val_loss: 0.3491\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4035 - val_loss: 0.3913\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.3848 - val_loss: 0.3437\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.3433 - val_loss: 0.3720\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3295 - val_loss: 0.2465\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2168 - val_loss: 0.2112\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2108 - val_loss: 0.2136\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2102 - val_loss: 0.2125\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2104 - val_loss: 0.2122\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2105 - val_loss: 0.2118\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2106 - val_loss: 0.2110\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2099 - val_loss: 0.2116\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2103 - val_loss: 0.2109\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2099 - val_loss: 0.2125\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2098 - val_loss: 0.2246\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2110 - val_loss: 0.2111\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2099 - val_loss: 0.2114\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2117 - val_loss: 0.2126\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2105\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2071 - val_loss: 0.2101\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2094\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2099\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2098\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2053 - val_loss: 0.2102\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2048 - val_loss: 0.2127\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2054 - val_loss: 0.2103\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2050 - val_loss: 0.2108\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2115\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2100\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2057 - val_loss: 0.2096\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2047 - val_loss: 0.2098\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2040 - val_loss: 0.2100\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2099\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2098\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2044 - val_loss: 0.2097\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2099\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2098\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2041 - val_loss: 0.2097\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2096\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2095\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2052 - val_loss: 0.2099\n",
      "Fold 3 NN: 0.20945\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 5s 18ms/step - loss: 31.1970 - val_loss: 1.0518\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.9894 - val_loss: 0.5830\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7209 - val_loss: 0.5131\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6264 - val_loss: 0.7268\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6184 - val_loss: 0.5617\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5559 - val_loss: 0.2461\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3138 - val_loss: 0.2517\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3007 - val_loss: 0.3115\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3032 - val_loss: 0.3787\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3164 - val_loss: 1.0451\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 5.1076 - val_loss: 0.2665\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2648 - val_loss: 0.2453\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2502 - val_loss: 0.2680\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2433 - val_loss: 0.2370\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2384 - val_loss: 0.3600\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2461 - val_loss: 0.2693\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2448 - val_loss: 0.2291\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2375 - val_loss: 0.2835\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2445 - val_loss: 0.2680\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2479 - val_loss: 0.2968\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2482 - val_loss: 0.2457\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2558 - val_loss: 0.4216\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2520 - val_loss: 0.2562\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2487 - val_loss: 0.3070\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2203 - val_loss: 0.2223\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2089 - val_loss: 0.2235\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2088 - val_loss: 0.2246\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2079 - val_loss: 0.2227\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2088 - val_loss: 0.2248\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2083 - val_loss: 0.2242\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2086 - val_loss: 0.2235\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2077 - val_loss: 0.2234\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2057 - val_loss: 0.2219\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2056 - val_loss: 0.2214\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2052 - val_loss: 0.2217\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2219\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2058 - val_loss: 0.2218\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2217\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2052 - val_loss: 0.2214\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2057 - val_loss: 0.2211\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2215\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2229\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2214\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2051 - val_loss: 0.2226\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2046 - val_loss: 0.2213\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2062 - val_loss: 0.2235\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2049 - val_loss: 0.2244\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2216\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2215\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2213\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2214\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2214\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2216\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2217\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2214\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2213\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2213\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2212\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2034 - val_loss: 0.2216\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2043 - val_loss: 0.2216\n",
      "Fold 4 NN: 0.22107\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 25.5301 - val_loss: 0.8127\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.8693 - val_loss: 0.6585\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6442 - val_loss: 0.5802\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5775 - val_loss: 0.4830\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5449 - val_loss: 0.5739\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5845 - val_loss: 0.4472\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4759 - val_loss: 0.3969\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4633 - val_loss: 0.5188\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3837 - val_loss: 0.2393\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2323 - val_loss: 0.2363\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2408 - val_loss: 0.2457\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2357 - val_loss: 0.2699\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2458 - val_loss: 0.2645\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2550 - val_loss: 0.2199\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2475 - val_loss: 0.2292\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2679 - val_loss: 0.3328\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2702 - val_loss: 0.2232\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3049 - val_loss: 0.2237\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2703 - val_loss: 0.2846\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2533 - val_loss: 0.2757\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.5643 - val_loss: 0.2590\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2305 - val_loss: 0.2242\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2198 - val_loss: 0.2216\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2173 - val_loss: 0.2214\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2201\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2154 - val_loss: 0.2186\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2148 - val_loss: 0.2277\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2148 - val_loss: 0.2185\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2133 - val_loss: 0.2169\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2122 - val_loss: 0.2181\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2112 - val_loss: 0.2358\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2154 - val_loss: 0.2165\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2113 - val_loss: 0.2178\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2124 - val_loss: 0.2173\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2110 - val_loss: 0.2168\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2111 - val_loss: 0.2195\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2099 - val_loss: 0.2183\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2135 - val_loss: 0.2168\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2092 - val_loss: 0.2171\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2072 - val_loss: 0.2153\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2148\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2064 - val_loss: 0.2153\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2059 - val_loss: 0.2153\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2148\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2142\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2154\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2147\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2196\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2189\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2060 - val_loss: 0.2152\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2147\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2148\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2140\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2043 - val_loss: 0.2138\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2149\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2140\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2042 - val_loss: 0.2139\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2054 - val_loss: 0.2145\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2034 - val_loss: 0.2140\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2151\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2141\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2043 - val_loss: 0.2137\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2138\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2137\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2136\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2141\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2041 - val_loss: 0.2138\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2043 - val_loss: 0.2139\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2043 - val_loss: 0.2140\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2137\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2138\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2036 - val_loss: 0.2139\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2051 - val_loss: 0.2139\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2040 - val_loss: 0.2137\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2139\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2041 - val_loss: 0.2138\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2043 - val_loss: 0.2138\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2139\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2138\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2138\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2035 - val_loss: 0.2138\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2138\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2138\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2041 - val_loss: 0.2139\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2138\n",
      "Fold 5 NN: 0.21361\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d10b441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:28:40.723994Z",
     "iopub.status.busy": "2021-09-27T19:28:40.723138Z",
     "iopub.status.idle": "2021-09-27T19:28:40.726463Z",
     "shell.execute_reply": "2021-09-27T19:28:40.726944Z"
    },
    "papermill": {
     "duration": 4.258888,
     "end_time": "2021-09-27T19:28:40.727105",
     "exception": false,
     "start_time": "2021-09-27T19:28:36.468217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00177634, 0.00269826, 0.00269826])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8d4b4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:28:49.720972Z",
     "iopub.status.busy": "2021-09-27T19:28:49.499674Z",
     "iopub.status.idle": "2021-09-27T19:42:45.000865Z",
     "shell.execute_reply": "2021-09-27T19:42:45.000261Z"
    },
    "papermill": {
     "duration": 840.076046,
     "end_time": "2021-09-27T19:42:45.001040",
     "exception": false,
     "start_time": "2021-09-27T19:28:44.924994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 32.6265 - val_loss: 1.3429\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7386 - val_loss: 0.5879\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6285 - val_loss: 0.4744\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5531 - val_loss: 0.5771\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5392 - val_loss: 0.5602\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5206 - val_loss: 0.6731\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5078 - val_loss: 0.4789\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4397 - val_loss: 0.7317\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4904 - val_loss: 0.4816\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4875 - val_loss: 0.2934\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2491 - val_loss: 0.2218\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2406 - val_loss: 0.2632\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2396 - val_loss: 0.2941\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2641 - val_loss: 0.2247\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2580 - val_loss: 0.2479\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6295 - val_loss: 0.5829\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2945 - val_loss: 0.2522\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2289 - val_loss: 0.2697\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2243 - val_loss: 0.2151\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2141 - val_loss: 0.2139\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2140 - val_loss: 0.2128\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2140 - val_loss: 0.2134\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2120 - val_loss: 0.2243\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2158 - val_loss: 0.2131\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2127 - val_loss: 0.2139\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2126\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 3s 19ms/step - loss: 0.2121 - val_loss: 0.2127\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2103 - val_loss: 0.2118\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2131 - val_loss: 0.2115\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2113 - val_loss: 0.2113\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2122 - val_loss: 0.2148\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2141\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2111\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2122 - val_loss: 0.2107\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2099 - val_loss: 0.2128\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2125 - val_loss: 0.2112\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2109 - val_loss: 0.2118\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2092 - val_loss: 0.2145\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2112 - val_loss: 0.2096\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2141 - val_loss: 0.2114\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 3s 20ms/step - loss: 0.2103 - val_loss: 0.2138\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 3s 19ms/step - loss: 0.2148 - val_loss: 0.2401\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2147 - val_loss: 0.2171\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2119 - val_loss: 0.2246\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2112 - val_loss: 0.2136\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2121 - val_loss: 0.2117\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2090\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2087\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2034 - val_loss: 0.2084\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2092\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2084\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2030 - val_loss: 0.2086\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2040 - val_loss: 0.2086\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2028 - val_loss: 0.2080\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2023 - val_loss: 0.2083\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2084\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2025 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2022 - val_loss: 0.2087\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2026 - val_loss: 0.2081\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2086\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2028 - val_loss: 0.2082\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2082\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2086\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2082\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2013 - val_loss: 0.2081\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2032 - val_loss: 0.2085\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2018 - val_loss: 0.2083\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2081\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2082\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2083\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2083\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2022 - val_loss: 0.2081\n",
      "Fold 1 NN: 0.20804\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 17ms/step - loss: 28.0011 - val_loss: 1.3079\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.1294 - val_loss: 1.0962\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.1551 - val_loss: 1.4138\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 1.1100 - val_loss: 1.3911\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 1.2390 - val_loss: 1.4773\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.9384 - val_loss: 1.1001\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.8284 - val_loss: 0.9086\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7230 - val_loss: 0.7307\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7005 - val_loss: 0.7804\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6176 - val_loss: 0.4720\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4948 - val_loss: 0.6173\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5168 - val_loss: 0.3045\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.4438 - val_loss: 0.2598\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2495 - val_loss: 0.3171\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2588 - val_loss: 0.2879\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2558 - val_loss: 0.2750\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.3045 - val_loss: 0.3083\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2758 - val_loss: 0.2344\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2733 - val_loss: 0.2436\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3452 - val_loss: 0.2696\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2836 - val_loss: 0.3428\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2891 - val_loss: 0.2465\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2603 - val_loss: 0.4127\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4546 - val_loss: 0.2371\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4361 - val_loss: 0.2459\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2130 - val_loss: 0.2187\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2101 - val_loss: 0.2180\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2091 - val_loss: 0.2163\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2087 - val_loss: 0.2203\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2076 - val_loss: 0.2257\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2078 - val_loss: 0.2142\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2074 - val_loss: 0.2159\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2090 - val_loss: 0.2176\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2068 - val_loss: 0.2183\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2063 - val_loss: 0.2131\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2070 - val_loss: 0.2154\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2257\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2089 - val_loss: 0.2162\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2236\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2078 - val_loss: 0.2223\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2072 - val_loss: 0.2167\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2069 - val_loss: 0.2144\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2016 - val_loss: 0.2128\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2018 - val_loss: 0.2141\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2154\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2139\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2152\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2012 - val_loss: 0.2146\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2142\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2016 - val_loss: 0.2169\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2136\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2140\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1998 - val_loss: 0.2136\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2127\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1998 - val_loss: 0.2144\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2002 - val_loss: 0.2135\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1998 - val_loss: 0.2143\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1996 - val_loss: 0.2135\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.1993 - val_loss: 0.2140\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2135\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.1998 - val_loss: 0.2137\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.1996 - val_loss: 0.2134\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 4s 26ms/step - loss: 0.1996 - val_loss: 0.2130\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.1991 - val_loss: 0.2132\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2137\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1990 - val_loss: 0.2134\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1996 - val_loss: 0.2137\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1992 - val_loss: 0.2137\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2135\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1990 - val_loss: 0.2135\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1992 - val_loss: 0.2136\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.1996 - val_loss: 0.2136\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1995 - val_loss: 0.2136\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1992 - val_loss: 0.2136\n",
      "Fold 2 NN: 0.21269\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 25.7640 - val_loss: 1.6009\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.0932 - val_loss: 0.7117\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6284 - val_loss: 0.5794\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6245 - val_loss: 0.4429\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5997 - val_loss: 0.5527\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6306 - val_loss: 0.6042\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6493 - val_loss: 0.2740\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2634 - val_loss: 0.2658\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2676 - val_loss: 0.2307\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2804 - val_loss: 0.3170\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.8344 - val_loss: 0.4394\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4796 - val_loss: 0.3879\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2972 - val_loss: 0.2910\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2503 - val_loss: 0.2199\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2310 - val_loss: 0.3627\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2863 - val_loss: 0.2329\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2265 - val_loss: 0.2360\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2269 - val_loss: 0.2322\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2382 - val_loss: 0.2869\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2580 - val_loss: 0.2166\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2398 - val_loss: 0.2479\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2402 - val_loss: 0.2720\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2534 - val_loss: 0.2167\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6325 - val_loss: 0.2419\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2440 - val_loss: 0.2198\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2273 - val_loss: 0.2843\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2345 - val_loss: 0.2217\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2145 - val_loss: 0.2156\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2134 - val_loss: 0.2127\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2124 - val_loss: 0.2165\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2115 - val_loss: 0.2138\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2122 - val_loss: 0.2142\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2111 - val_loss: 0.2132\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2131\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2110 - val_loss: 0.2119\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2101 - val_loss: 0.2133\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2106 - val_loss: 0.2145\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2120 - val_loss: 0.2145\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2104 - val_loss: 0.2129\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2143 - val_loss: 0.2119\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2105 - val_loss: 0.2175\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2134 - val_loss: 0.2134\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2078 - val_loss: 0.2104\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2067 - val_loss: 0.2109\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2061 - val_loss: 0.2119\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2061 - val_loss: 0.2106\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2067 - val_loss: 0.2108\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2110\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2062 - val_loss: 0.2111\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2114\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2106\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2108\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2106\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2105\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2045 - val_loss: 0.2107\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2111\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2106\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2106\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2050 - val_loss: 0.2105\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2107\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2106\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2046 - val_loss: 0.2107\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2055 - val_loss: 0.2105\n",
      "Fold 3 NN: 0.21041\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 29.7994 - val_loss: 1.2564\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.2352 - val_loss: 1.1471\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7251 - val_loss: 0.9808\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7377 - val_loss: 1.0495\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.8458 - val_loss: 0.6815\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.8462 - val_loss: 0.5438\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6264 - val_loss: 0.6711\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.6010 - val_loss: 0.5233\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.5481 - val_loss: 0.7160\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.6194 - val_loss: 0.5106\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.4770 - val_loss: 0.3650\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6200 - val_loss: 0.6110\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4465 - val_loss: 0.4897\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4139 - val_loss: 0.9486\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0156 - val_loss: 0.2953\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3497 - val_loss: 0.3876\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4372 - val_loss: 0.3304\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3302 - val_loss: 0.6437\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3818 - val_loss: 0.3163\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3227 - val_loss: 0.4182\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3109 - val_loss: 0.2390\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2324 - val_loss: 0.2202\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2207 - val_loss: 0.2240\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2246 - val_loss: 0.2537\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2465 - val_loss: 0.2378\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2266 - val_loss: 0.2220\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2335 - val_loss: 0.2439\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2324 - val_loss: 0.2307\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2378 - val_loss: 0.2261\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2073 - val_loss: 0.2179\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2160\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2169\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2054 - val_loss: 0.2154\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2155\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2151\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2052 - val_loss: 0.2172\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2175\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2085 - val_loss: 0.2162\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2049 - val_loss: 0.2146\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2050 - val_loss: 0.2260\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2066 - val_loss: 0.2267\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2065 - val_loss: 0.2230\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2177\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2286\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2079 - val_loss: 0.2203\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2058 - val_loss: 0.2226\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2156\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2007 - val_loss: 0.2169\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2007 - val_loss: 0.2160\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2004 - val_loss: 0.2156\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2157\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2002 - val_loss: 0.2154\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2000 - val_loss: 0.2160\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2157\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1992 - val_loss: 0.2152\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1989 - val_loss: 0.2157\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1996 - val_loss: 0.2156\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1991 - val_loss: 0.2156\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2163\n",
      "Fold 4 NN: 0.21457\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 16.7685 - val_loss: 1.0515\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.9996 - val_loss: 0.8373\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.8118 - val_loss: 0.8507\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7486 - val_loss: 1.0024\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6369 - val_loss: 0.5955\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4385 - val_loss: 0.2849\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.0694 - val_loss: 0.5487\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5107 - val_loss: 0.4509\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.4591 - val_loss: 0.6235\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5300 - val_loss: 0.3901\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 4s 26ms/step - loss: 0.4202 - val_loss: 0.4792\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 3s 20ms/step - loss: 0.4012 - val_loss: 0.4064\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.3883 - val_loss: 0.4308\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2480 - val_loss: 0.2245\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2137 - val_loss: 0.2208\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2128 - val_loss: 0.2199\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2125 - val_loss: 0.2186\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2213\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2114 - val_loss: 0.2181\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2217\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2108 - val_loss: 0.2178\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2113 - val_loss: 0.2243\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2168 - val_loss: 0.2175\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2154 - val_loss: 0.2223\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2126 - val_loss: 0.2214\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2150 - val_loss: 0.2171\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2098 - val_loss: 0.2224\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2159 - val_loss: 0.2231\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2140 - val_loss: 0.2223\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2111 - val_loss: 0.2179\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2110 - val_loss: 0.2150\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2116 - val_loss: 0.2215\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2093 - val_loss: 0.2404\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2246 - val_loss: 0.2300\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2219 - val_loss: 0.2202\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2080 - val_loss: 0.2184\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2101 - val_loss: 0.2331\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2108 - val_loss: 0.2412\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2064 - val_loss: 0.2134\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2153\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2023 - val_loss: 0.2165\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2134\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2022 - val_loss: 0.2177\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2013 - val_loss: 0.2172\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2139\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2169\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2147\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1999 - val_loss: 0.2139\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2150\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2000 - val_loss: 0.2139\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1996 - val_loss: 0.2140\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1997 - val_loss: 0.2135\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2150\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1998 - val_loss: 0.2139\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.1995 - val_loss: 0.2138\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1997 - val_loss: 0.2144\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1994 - val_loss: 0.2139\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2004 - val_loss: 0.2139\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1995 - val_loss: 0.2138\n",
      "Fold 5 NN: 0.21342\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(41)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(41)\n",
    "from tensorflow import keras\n",
    "\n",
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train1)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "test1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "\n",
    "train1[pred_name] = 0\n",
    "test1[target_name] = 0\n",
    "test_predictions_nn1 = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n",
    "    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8dfdbf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:43:01.254481Z",
     "iopub.status.busy": "2021-09-27T19:43:01.253720Z",
     "iopub.status.idle": "2021-09-27T19:43:01.280987Z",
     "shell.execute_reply": "2021-09-27T19:43:01.281477Z"
    },
    "papermill": {
     "duration": 8.228274,
     "end_time": "2021-09-27T19:43:01.281663",
     "exception": false,
     "start_time": "2021-09-27T19:42:53.053389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_price1_max</th>\n",
       "      <th>bid_price1_min</th>\n",
       "      <th>bid_price1_last</th>\n",
       "      <th>bid_price1_first</th>\n",
       "      <th>bid_price2_max</th>\n",
       "      <th>bid_price2_min</th>\n",
       "      <th>bid_price2_last</th>\n",
       "      <th>bid_price2_first</th>\n",
       "      <th>ask_price1_max</th>\n",
       "      <th>ask_price1_min</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_6c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_6c1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.725806</td>\n",
       "      <td>1.107053</td>\n",
       "      <td>0.111474</td>\n",
       "      <td>0.299485</td>\n",
       "      <td>-0.891856</td>\n",
       "      <td>0.948976</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.052241</td>\n",
       "      <td>-0.615611</td>\n",
       "      <td>1.131312</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199337</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.217304</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.084157</td>\n",
       "      <td>3.135875</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>-0.158116</td>\n",
       "      <td>-0.479887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218283</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.217304</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.084157</td>\n",
       "      <td>-0.082936</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>-0.158116</td>\n",
       "      <td>-0.479887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218283</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.217304</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.084157</td>\n",
       "      <td>-0.082936</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>-0.158116</td>\n",
       "      <td>-0.479887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bid_price1_max  bid_price1_min  bid_price1_last  bid_price1_first  \\\n",
       "0       -0.725806        1.107053         0.111474          0.299485   \n",
       "1        0.000041       -0.001605        -0.000537          0.001668   \n",
       "2        0.000041       -0.001605        -0.000537          0.001668   \n",
       "\n",
       "   bid_price2_max  bid_price2_min  bid_price2_last  bid_price2_first  \\\n",
       "0       -0.891856        0.948976         0.008407          0.052241   \n",
       "1       -0.000268       -0.001916        -0.000681          0.002108   \n",
       "2       -0.000268       -0.001916        -0.000681          0.002108   \n",
       "\n",
       "   ask_price1_max  ask_price1_min  ...  bid_ask_spread_sum_1c1  \\\n",
       "0       -0.615611        1.131312  ...               -5.199337   \n",
       "1        0.000616       -0.000118  ...               -0.218283   \n",
       "2        0.000616       -0.000118  ...               -0.218283   \n",
       "\n",
       "   bid_ask_spread_sum_3c1  bid_ask_spread_sum_4c1  bid_ask_spread_sum_6c1  \\\n",
       "0                0.032216                0.217304                0.427136   \n",
       "1                0.032216                0.217304                0.427136   \n",
       "2                0.032216                0.217304                0.427136   \n",
       "\n",
       "   size_tau2_0c1  size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_6c1  \\\n",
       "0      -0.084157       3.135875       0.593047      -0.158116      -0.479887   \n",
       "1      -0.084157      -0.082936       0.593047      -0.158116      -0.479887   \n",
       "2      -0.084157      -0.082936       0.593047      -0.158116      -0.479887   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "\n",
       "[3 rows x 287 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf12101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T19:43:17.729252Z",
     "iopub.status.busy": "2021-09-27T19:43:17.728582Z",
     "iopub.status.idle": "2021-09-27T19:43:17.762269Z",
     "shell.execute_reply": "2021-09-27T19:43:17.762786Z"
    },
    "papermill": {
     "duration": 8.255929,
     "end_time": "2021-09-27T19:43:17.762973",
     "exception": false,
     "start_time": "2021-09-27T19:43:09.507044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.002477\n",
       "1   0-32  0.002735\n",
       "2   0-34  0.002735"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "a=test_predictions_nn*0.6+predictions_lgb2*0.4 #0.6 0.4\n",
    "b=test_predictions_nn1*0.6+predictions_lgb1*0.4 # 0.55 0.45\n",
    "#test[target_name] = (a+b)/2\n",
    "c=test_predictions_nn*0.6+predictions_cat*0.4\n",
    "#d=test_predictions_nn1*0.56+predictions_lgb*0.44\n",
    "test[target_name] = (a+b+c)/3\n",
    "\n",
    "#test[target_name] = (test_predictions_nn*0.4+predictions_cat*0.3+predictions_lgb*0.3)/3\n",
    "\n",
    "#a = test_predictions_nn*0.6+predictions_cat*0.4\n",
    "#b = test_predictions_nn1*0.55+predictions_cat*0.45\n",
    "#test[target_name] = (a+b)/2\n",
    "\n",
    "display(test[['row_id', target_name]].head(3))\n",
    "test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3e49b",
   "metadata": {
    "papermill": {
     "duration": 8.167239,
     "end_time": "2021-09-27T19:43:34.116703",
     "exception": false,
     "start_time": "2021-09-27T19:43:25.949464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "references:\n",
    "\n",
    "1. https://www.kaggle.com/shivansh002/hit-and-trial-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55aa168",
   "metadata": {
    "papermill": {
     "duration": 8.149878,
     "end_time": "2021-09-27T19:43:50.413717",
     "exception": false,
     "start_time": "2021-09-27T19:43:42.263839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12644.136543,
   "end_time": "2021-09-27T19:44:00.843108",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T16:13:16.706565",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
